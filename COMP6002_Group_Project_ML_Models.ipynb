{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MGMnmhjzT-73",
        "eEz2Qzc_kzkt",
        "gSVADpSUPvxs",
        "f0fI9HD0RaaX",
        "2IH9h-H8QULd",
        "qZskn0e5p4qF",
        "sTh9xpbpp8Gm",
        "60pYh9NWQbIB"
      ],
      "authorship_tag": "ABX9TyP6/5NS7O6mMqBJP8RFNlUE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frosk-Kristian/COMP6002-Group10-Models/blob/develop/COMP6002_Group_Project_ML_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMP6002 Computer Science Project - Group 10\n",
        "Utilising Machine Learning to detect DDoS attacks.\n",
        "\n",
        "## Reference\n",
        "Iman Sharafaldin, Arash Habibi Lashkari, Saqib Hakak, and Ali A. Ghorbani, \"Developing Realistic Distributed Denial of Service (DDoS) Attack Dataset and Taxonomy\", IEEE 53rd International Carnahan Conference on Security Technology, Chennai, India, 2019."
      ],
      "metadata": {
        "id": "vkidwNbBPly0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "Run all of these first. Sets up libraries and directories used throughout notebook."
      ],
      "metadata": {
        "id": "zN1_GsQjkwEd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgQiJbJiPMYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a40a53-70b2-4207-b8ef-008ddc9d7899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Pandas version: 2.0.3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "print(f\"Using Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(f\"Using Numpy version: {np.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL7Gvu83G5FF",
        "outputId": "7195715d-128d-4813-e93c-427eeadfac04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Numpy version: 1.25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import __version__ as skl_ver\n",
        "print(f\"Using Sklearn version: {skl_ver}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg4eRaq0fNry",
        "outputId": "8ec35009-28d3-456e-c1b2-ddac2c9d0c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Sklearn version: 1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install codecarbon\n",
        "\n",
        "from codecarbon import EmissionsTracker\n",
        "from codecarbon import __version__ as cc_ver\n",
        "print(f\"Using CodeCarbon.io version: {cc_ver}\")\n",
        "\n",
        "# track project emissions\n",
        "tracker = EmissionsTracker()\n",
        "tracker.start()"
      ],
      "metadata": {
        "id": "kB8X6j51PPET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# mounts google drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# directory that all exports will be stored in\n",
        "dl_dir = os.getcwd() + r'/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data'\n",
        "# directory that the runtime will store the unzipped dataset in (not on your drive)\n",
        "data_dir = os.getcwd() + '/dataset'\n",
        "\n",
        "# checks if export directory already exists, if not creates it\n",
        "if os.path.exists(dl_dir):\n",
        "  print(f\"Directory {dl_dir} already exists.\\n\")\n",
        "else:\n",
        "  os.mkdir(dl_dir)\n",
        "  print(f\"Successfully created the directory {dl_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUdWg_050_Ro",
        "outputId": "8bce2987-3c43-40d1-dedd-fcc268f706fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Directory /content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data already exists.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance Specs\n",
        "Gets hardware specifications of the Google Colab instance."
      ],
      "metadata": {
        "id": "MGMnmhjzT-73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU specifications\n",
        "from psutil import *\n",
        "\n",
        "print(f\"Number of CPU: {cpu_count()}\")\n",
        "\n",
        "!cat /proc/cpuinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW7ISKOxJFGT",
        "outputId": "34e7c283-b4c9-4360-de54-4c5f7247da84"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CPU: 2\n",
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VRAM specifications\n",
        "virtual_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LVrEUT0Ja_7",
        "outputId": "48c8d75d-266d-41cc-98a1-5a1f066396e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "svmem(total=13609451520, available=12429377536, percent=8.7, used=873287680, free=8717107200, active=612868096, inactive=4022714368, buffers=344756224, cached=3674300416, shared=1409024, slab=154161152)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU specifications\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM8VQOFEUKZ0",
        "outputId": "0a2213ee-9678-4a96-91d0-ee7b7c9a5c85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Helper Functions"
      ],
      "metadata": {
        "id": "eEz2Qzc_kzkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions for exporting/importing models trained with Sklearn, do not attempt to use with Neural Network as the library used has its' own methods for exporting/importing\n",
        "# all functions defined will prompt the user for confirmation, to allow for skipping the functions when running the notebook\n",
        "import joblib\n",
        "print(f\"Using Joblib version: {joblib.__version__}\")\n",
        "\n",
        "# function used to append a number to a filename in the event that the file already exists\n",
        "def IncrementFname(file_path):\n",
        "  \"\"\"\n",
        "  Takes a file path, splits the file name/extension and inserts an incrementing integer between them until a unique name is found.\n",
        "\n",
        "  Parameters:\n",
        "    file_path (string): full file path to increment, including extension.\n",
        "  Returns:\n",
        "    string: new file name in the form of 'path/to/file(i).extension'.\n",
        "  \"\"\"\n",
        "  fname, ext = os.path.splitext(file_path)\n",
        "  i = 1\n",
        "  new_name = f'{fname}({i}){ext}' # intial reassignment\n",
        "\n",
        "  # if filename(1).extension already exists, increment number until an unused one is found\n",
        "  while os.path.exists(new_name):\n",
        "    i += 1\n",
        "    new_name = f'{fname}({i}){ext}'\n",
        "\n",
        "  return new_name\n",
        "\n",
        "# Save a trained model to the provided filepath\n",
        "def SaveSKL(model, model_fpath):\n",
        "  \"\"\"\n",
        "  Exports a trained model via joblib.\n",
        "\n",
        "  Parameters:\n",
        "    model (object): model to be saved.\n",
        "    model_fpath (string): file path that the model will be saved in, including filename and extension.\n",
        "  Returns:\n",
        "    : no value returned.\n",
        "  \"\"\"\n",
        "  to_save = \"\"\n",
        "\n",
        "  while to_save.lower() not in ('y', 'n'):\n",
        "    to_save = input(\"Do you wish to save the trained model? (y/n)\\n\")\n",
        "    if to_save.lower() in 'y':\n",
        "      fpath = model_fpath # assigns value to new variable to allow for reassignment\n",
        "\n",
        "      if os.path.isfile(fpath):\n",
        "        print(f\"File \\'{fpath}\\' already exists...\")\n",
        "        fpath = IncrementFname(fpath)\n",
        "        print(f\"...Using the name \\'{fpath}\\' instead.\")\n",
        "\n",
        "      print(f\"Saving model to: {fpath}\")\n",
        "      try:\n",
        "        joblib.dump(model, fpath)\n",
        "        print(f\"SUCCESS: Model saved to {fpath}\")\n",
        "      except:\n",
        "        print(\"ERROR: An unknown error has occured when calling joblib.dump()!\")\n",
        "    else:\n",
        "      if to_save.lower() in 'n':\n",
        "        print(\"Did not save model.\")\n",
        "      else:\n",
        "        print(\"Please only enter \\'y\\' to save model or \\'n\\' to skip saving.\")\n",
        "\n",
        "# Load a trained model from the provided filepath\n",
        "def LoadSKL(model_fpath):\n",
        "  \"\"\"\n",
        "  Import a trained model via joblib.\n",
        "\n",
        "  Parameters:\n",
        "    model_fpath (string): file path to the stored model.\n",
        "  Returns:\n",
        "    object: if a model is found and loaded correctly, returns an object.\n",
        "    None: if no matching file is found or an error occurs during loading, returns None.\n",
        "  \"\"\"\n",
        "  to_load = \"\"\n",
        "  while to_load.lower() not in ('y', 'n'):\n",
        "    to_load = input(\"Do you wish to import a trained model? {y/n)\\n\")\n",
        "    if to_load.lower() in 'y':\n",
        "      model = None\n",
        "      print(f\"Attempting to import model from: {model_fpath}\")\n",
        "      try:\n",
        "        model = joblib.load(model_fpath)\n",
        "        print(\"SUCCESS: Model successfully imported.\")\n",
        "      except FileNotFoundError:\n",
        "        print(f\"ERROR: The file \\'{model_fpath}\\' does not exist!\")\n",
        "        model = None\n",
        "      except:\n",
        "        print(\"ERROR: An unknown error has occured when calling joblib.load()!\")\n",
        "        model = None\n",
        "      finally:\n",
        "        return model\n",
        "    else:\n",
        "      if to_load.lower() in 'n':\n",
        "        print(\"Did not import model.\")\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Please only enter \\'y\\' to import model or \\'n\\' to skip importing.\")\n",
        "\n",
        "# To-do: write function that exports model parameters, evaluation metrics, etc."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otJrdxCQXWv5",
        "outputId": "891c52fb-4ca3-4c3d-8bfd-b95b14a3f177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Joblib version: 1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data\n",
        "Checks current working directory for datasets, if datasets are missing downloads a [.zip archive mirror of the CiCDDoS2019 hosted on Kaggle](https://www.kaggle.com/datasets/kristianfrossos/cicddos2019/data).\n",
        "\n",
        "**NOTE:** the first part of this section is specific to Google Colab, and will not work outside of it. Advise writing an alternative later for local use (relevant when training Neural Network for speed and when usage limits get in the way).\n"
      ],
      "metadata": {
        "id": "gSVADpSUPvxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "rVY0RkBfOV1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sets up kaggle environment variables (needed to access API)\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "\n",
        "# checks if kaggle key and username have been provided as secrets and sets environment variables appropriately\n",
        "# if not found, attempts to use kaggle.json\n",
        "try:\n",
        "  os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "  os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "  print(\"Using KAGGLE_KEY and KAGGLE_USERNAME defined in secrets.\")\n",
        "except (userdata.SecretNotFoundError, userdata.NotebookAccessError):\n",
        "  print(\"WARN: One or more secret(s) missing or inaccessible.\\n\")\n",
        "  if os.path.isfile('~/.kaggle/kaggle.json'):\n",
        "    print(\"Using existing kaggle.json\")\n",
        "  else:\n",
        "    print(\"Please upload kaggle.json\")\n",
        "    files.upload()\n",
        "\n",
        "    if os.path.isfile(os.getcwd() + '/content/kaggle.json'):\n",
        "      !rm -r ~/.kaggle\n",
        "      !mkdir ~/.kaggle\n",
        "      !mv ./kaggle.json ~/.kaggle/\n",
        "      !chmod 600 ~/.kaggle/kaggle.json\n",
        "    else:\n",
        "      print(\"\\'kaggle.json\\' not uploaded.\")\n",
        "      raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcOT1v4TO4rm",
        "outputId": "bd65b3e9-151d-4045-e099-a3a412c8cf36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using KAGGLE_KEY and KAGGLE_USERNAME defined in secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checks if .zip archive containing dataset already exists in google drive and downloads it if necessary\n",
        "if os.path.isfile(dl_dir + '/cicddos2019.zip'):\n",
        "  print(\"Dataset already present.\")\n",
        "else:\n",
        "  print(f\"Downloading zipped dataset to {dl_dir}\")\n",
        "  !kaggle datasets download kristianfrossos/cicddos2019 -p {dl_dir.replace(' ', '\\ ')}\n",
        "\n",
        "# creates the content/dataset directory if it doesn't already exist\n",
        "if os.path.exists(data_dir):\n",
        "  print(f\"Directory {data_dir} already exists.\")\n",
        "else:\n",
        "  print(f\"Created directory: {data_dir}\")\n",
        "  os.mkdir(data_dir)\n",
        "\n",
        "# extracts contents of .zip archive to content/dataset if directory is not empty\n",
        "if not os.listdir(data_dir):\n",
        "  print(\"Empty directory, extracting dataset.\")\n",
        "  # unzips .zip archive\n",
        "  !unzip {dl_dir.replace(' ', '\\ ') + '/cicddos2019.zip'} -d {data_dir}\n",
        "else:\n",
        "  print(\"Non-empty directory, skipping download\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUQvxpnNMnRQ",
        "outputId": "7facfb54-291b-4eb5-9a8e-87b1871422b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already present.\n",
            "Directory /content/dataset already exists.\n",
            "Non-empty directory, skipping download\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialises empty list\n",
        "csv_list = []\n",
        "\n",
        "# iterates through all subdirectories of /content/dataset and appends the filepath of each .csv to csv_list\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "  for f in files:\n",
        "    if f.endswith(\".csv\"):\n",
        "      csv_list.append(os.path.join(root, f))\n",
        "\n",
        "# if .csv files were found, displays number of files and prints each path\n",
        "if not csv_list:\n",
        "  print(\"No .csv files found!\")\n",
        "else:\n",
        "  print(f\"{len(csv_list)} .csv files found.\")\n",
        "  for csv in csv_list:\n",
        "    print(csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMw9ryJSbiPL",
        "outputId": "edb94f2d-c646-4b12-be13-5fd3845d11c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18 .csv files found.\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_MSSQL.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_DNS.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_SNMP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_LDAP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_NetBIOS.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_NTP.csv\n",
            "/content/dataset/CSV-01-12/01-12/TFTP.csv\n",
            "/content/dataset/CSV-01-12/01-12/Syn.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_SSDP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_UDP.csv\n",
            "/content/dataset/CSV-01-12/01-12/UDPLag.csv\n",
            "/content/dataset/CSV-03-11/03-11/NetBIOS.csv\n",
            "/content/dataset/CSV-03-11/03-11/Portmap.csv\n",
            "/content/dataset/CSV-03-11/03-11/UDP.csv\n",
            "/content/dataset/CSV-03-11/03-11/Syn.csv\n",
            "/content/dataset/CSV-03-11/03-11/LDAP.csv\n",
            "/content/dataset/CSV-03-11/03-11/UDPLag.csv\n",
            "/content/dataset/CSV-03-11/03-11/MSSQL.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Samples Dataset and Creates Subset"
      ],
      "metadata": {
        "id": "f0fI9HD0RaaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ddos_df = pd.DataFrame()\n",
        "benign_df = pd.DataFrame()\n",
        "\n",
        "for csv in csv_list:\n",
        "  data_iter = pd.read_csv(csv, chunksize=2000)\n",
        "  print(f\"Reading {csv}...\")\n",
        "\n",
        "  for chunk in data_iter:\n",
        "    ddos_rows = chunk[chunk[' Label'].str.lower() != 'benign']\n",
        "    benign_rows = chunk[chunk[' Label'].str.lower() == 'benign']\n",
        "\n",
        "    sample_size = min(len(ddos_rows), len(benign_rows)) // 100\n",
        "    ddos_sample = ddos_rows.sample(n=sample_size, random_state=42)\n",
        "    benign_sample = benign_rows.sample(n=sample_size, random_state=42)\n",
        "\n",
        "    ddos_df = pd.concat([ddos_df, ddos_sample], ignore_index=True)\n",
        "    benign_df = pd.concat([benign_df, benign_sample], ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14YJSfXt7F0Z",
        "outputId": "654fc8c7-a7d3-4994-98de-15b81a8cb66b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_MSSQL.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_DNS.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_SNMP.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_LDAP.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_NetBIOS.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_NTP.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/TFTP.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/Syn.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_SSDP.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_UDP.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/UDPLag.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/NetBIOS.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/Portmap.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/UDP.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/Syn.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/LDAP.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/UDPLag.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/MSSQL.csv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenates ddos and benign dataframes into one subset\n",
        "subset = pd.concat([ddos_df, benign_df], ignore_index=True)\n",
        "\n",
        "# class weights\n",
        "weights = subset.value_counts(' Label', normalize=True)\n",
        "counts = subset.value_counts(' Label')\n",
        "\n",
        "# prints classes and their weights\n",
        "print(\"     Class      |     Weight     |     Count\")\n",
        "for index in weights.index:\n",
        "  print(f'{index:<15} | {(weights[index] * 100.0):<14n} | {counts[index]:n}')\n",
        "\n",
        "# prints total (for error checking)\n",
        "print(f\"{'Total':<15} | {sum(weights.values) * 100.0:<14n} | {sum(counts.values)}\")\n",
        "\n",
        "# stores unprocessed subset\n",
        "subset.to_csv((dl_dir + '/COMP6002_Raw_Subset.csv'), index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwKDB0qpF4Xz",
        "outputId": "4905415b-2e69-454c-bb33-714ebfd4ad6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Class      |     Weight     |     Count\n",
            "BENIGN          | 50             | 330\n",
            "Syn             | 11.8182        | 78\n",
            "TFTP            | 11.6667        | 77\n",
            "DrDoS_NTP       | 5.60606        | 37\n",
            "UDP-lag         | 4.24242        | 28\n",
            "DrDoS_DNS       | 2.57576        | 17\n",
            "UDPLag          | 2.42424        | 16\n",
            "MSSQL           | 2.27273        | 15\n",
            "UDP             | 1.9697         | 13\n",
            "Portmap         | 1.81818        | 12\n",
            "NetBIOS         | 1.36364        | 9\n",
            "DrDoS_UDP       | 1.06061        | 7\n",
            "DrDoS_MSSQL     | 0.909091       | 6\n",
            "LDAP            | 0.757576       | 5\n",
            "WebDDoS         | 0.606061       | 4\n",
            "DrDoS_SNMP      | 0.454545       | 3\n",
            "DrDoS_NetBIOS   | 0.454545       | 3\n",
            "Total           | 100            | 660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "**To-Do:**\n",
        "*   Identify features that can be droped.\n",
        "\n",
        "## Dropped Features\n",
        "*   Unnamed: 0: unknown feature.\n",
        "*   Flow Id: constructed from Source Ip, Destination Ip, Source Port, Destination Port and Protocol.\n",
        "*   Similar HTTP: object with no meaningful way to encode (the exact same objects won't necessaril exist in real data)"
      ],
      "metadata": {
        "id": "2IH9h-H8QULd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optionally import a unprocessed subset, skips above steps.\n",
        "raw_load = \"\"\n",
        "\n",
        "while raw_load.lower() not in ('y', 'n'):\n",
        "  raw_load = input(\"Do you wish to load a previously generated (but not processed) subset? (y/n)\\n\")\n",
        "\n",
        "  if raw_load.lower() in ('y'):\n",
        "    raw_path = dl_dir + '/COMP6002_Raw_Subset.csv'\n",
        "    try:\n",
        "      subset = pd.read_csv(raw_path)\n",
        "      print(f\"\\'{raw_path}\\' loaded successfully.\")\n",
        "    except:\n",
        "      print(f\"ERROR: an unknown error occured loading \\'{raw_path}\\'!\")\n",
        "  else:\n",
        "    if raw_load.lower() in ('n'):\n",
        "      print(\"Did not attempt to load existing data.\")\n",
        "    else:\n",
        "      print(\"WARNING: please only input \\'y\\' to load the data or \\'n\\' to skip loading.\")\n",
        "\n",
        "del raw_load"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAUF9N90JvnX",
        "outputId": "e08073c5-1b83-467a-85b4-c5e1a6b988bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you wish to load a previously generated (but not processed) subset? (y/n)\n",
            "y\n",
            "'/content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data/COMP6002_Raw_Subset.csv' loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import ipaddress\n",
        "\n",
        "# drops irrelevant columns\n",
        "subset.drop(columns = ['Unnamed: 0', 'Flow ID', 'SimillarHTTP'],\n",
        "            inplace = True)\n",
        "\n",
        "# replace infinite values with NaN so they are caught by the next 2 steps\n",
        "subset.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
        "\n",
        "# drop columns with at least 50% missing values\n",
        "subset.dropna(axis = 1,\n",
        "              thresh = int(0.5 * subset.shape[0]),\n",
        "              inplace = True)\n",
        "\n",
        "# replace missing values with the mean of their columns\n",
        "for col in subset.columns:\n",
        "  if subset[col].isna().sum() > 0:\n",
        "    subset[col].fillna(subset[col].mean(), inplace = True)\n",
        "\n",
        "# drop duplicate rows\n",
        "subset.drop_duplicates(inplace = True)\n",
        "\n",
        "# converts source and destination IP addresses to useable integer values\n",
        "subset['Source IP_int'] = subset.apply(lambda x: int (ipaddress.IPv4Address(x[' Source IP'])), axis=1)\n",
        "subset['Destination IP_int'] = subset.apply(lambda x: int (ipaddress.IPv4Address(x[' Destination IP'])), axis=1)\n",
        "\n",
        "# converts date and time values to unix timestamps\n",
        "subset['UnixTimestamp'] = subset.apply(lambda x: (pd.to_datetime(x[' Timestamp']).timestamp()), axis=1)\n",
        "\n",
        "# drops original columns\n",
        "subset.drop(columns = [' Source IP', ' Destination IP', ' Timestamp'],\n",
        "            inplace = True)\n",
        "\n",
        "# splits subset across x and y axis\n",
        "X = subset.drop(columns = [' Label'], inplace = False)\n",
        "y = subset[' Label']"
      ],
      "metadata": {
        "id": "Ncl7SwVoSldZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_path = dl_dir + '/COMP6002_Processed_Subset.csv'"
      ],
      "metadata": {
        "id": "fV6ZV38QoKBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Processed Data"
      ],
      "metadata": {
        "id": "qZskn0e5p4qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optionally save processed subset to .csv\n",
        "subset_save = \"\"\n",
        "\n",
        "while subset_save.lower() not in ('y', 'n'):\n",
        "  subset_save = input(\"Do you wish to export the subset to a .csv? (y/n)\")\n",
        "\n",
        "  if subset_save.lower() in ('y'):\n",
        "    new_subset_path = subset_path\n",
        "\n",
        "    if os.path.exists(new_subset_path):\n",
        "      new_subset_path = IncrementFname(subset_path)\n",
        "\n",
        "    try:\n",
        "      subset.to_csv(new_subset_path, index = False)\n",
        "      print(f\"Subset saved as \\'{new_subset_path}\\'.\")\n",
        "    except:\n",
        "      print(f\"ERROR: an unknown error occured saving \\'{new_subset_path}\\'!\")\n",
        "  else:\n",
        "    if subset_save.lower() in ('n'):\n",
        "      print(\"Skipped exporting to .csv file.\")\n",
        "    else:\n",
        "      print(\"WARNING: please only input \\'y\\' to save the data to a .csv or \\'n\\' to skip saving.\")\n",
        "\n",
        "del subset_save"
      ],
      "metadata": {
        "id": "WVs8OfgonhT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7866b803-1b11-4630-c4c7-29099276c1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you wish to export the subset to a .csv? (y/n)y\n",
            "Subset saved as '/content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data/COMP6002_Processed_Subset.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Previously Processed Data"
      ],
      "metadata": {
        "id": "sTh9xpbpp8Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optionally import a preprocessed subset, skips above steps.\n",
        "subset_load = \"\"\n",
        "\n",
        "while subset_load.lower() not in ('y', 'n'):\n",
        "  subset_load = input(\"Do you wish to load a previously preprocessed subset? (y/n)\\n\")\n",
        "\n",
        "  if subset_load.lower() in ('y'):\n",
        "    subset_path = dl_dir + '/COMP6002_Processed_Subset.csv'\n",
        "    try:\n",
        "      subset = pd.read_csv(subset_path)\n",
        "      print(f\"\\'{subset_path}\\' loaded successfully.\")\n",
        "    except:\n",
        "      print(f\"ERROR: an unknown error occured loading \\'{subset_path}\\'!\")\n",
        "  else:\n",
        "    if subset_load.lower() in ('n'):\n",
        "      print(\"Did not attempt to load existing data.\")\n",
        "    else:\n",
        "      print(\"WARNING: please only input \\'y\\' to load the data or \\'n\\' to skip loading.\")\n",
        "\n",
        "del subset_load"
      ],
      "metadata": {
        "id": "SXVkn2lbDsTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18b39b0-a978-48c8-e205-0d2175e37836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you wish to load a previously preprocessed subset? (y/n)\n",
            "n\n",
            "Did not attempt to load existing data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Training/Testing Data"
      ],
      "metadata": {
        "id": "XIZGXSiAqAHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set aside 20% of data to be used in testing, keeping the remaining 80% for training\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
      ],
      "metadata": {
        "id": "J2PH-oD5ljS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalise Data"
      ],
      "metadata": {
        "id": "zBp80dzJI3NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train[X_train.columns] = scaler.transform(X_train[X_train.columns])"
      ],
      "metadata": {
        "id": "7pnJ7yPbI5b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection"
      ],
      "metadata": {
        "id": "Qe8PpKdulNs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectPercentile\n",
        "\n",
        "selector = SelectPercentile(percentile=50).fit(X_train[X_train.columns], y_train)\n",
        "\n",
        "X_new = X[selector.get_feature_names_out()]\n",
        "\n",
        "print(f\"Features remaining: {X_new.shape[1]}\")\n",
        "for col in X_new.columns:\n",
        "  print(f\"{col}\")"
      ],
      "metadata": {
        "id": "koTZs8LAlSVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619714fd-5795-40a5-b7da-22f71961fe3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features remaining: 42\n",
            " Source Port\n",
            " Destination Port\n",
            " Protocol\n",
            " Flow Duration\n",
            " Total Fwd Packets\n",
            "Total Length of Fwd Packets\n",
            " Fwd Packet Length Max\n",
            " Fwd Packet Length Min\n",
            " Fwd Packet Length Mean\n",
            " Bwd Packet Length Min\n",
            " Bwd Packet Length Mean\n",
            " Bwd Packet Length Std\n",
            "Flow Bytes/s\n",
            " Flow Packets/s\n",
            "Fwd IAT Total\n",
            "Fwd PSH Flags\n",
            " Fwd Header Length\n",
            "Fwd Packets/s\n",
            " Min Packet Length\n",
            " Max Packet Length\n",
            " Packet Length Mean\n",
            " Packet Length Std\n",
            " RST Flag Count\n",
            " ACK Flag Count\n",
            " URG Flag Count\n",
            " CWE Flag Count\n",
            " Down/Up Ratio\n",
            " Average Packet Size\n",
            " Avg Fwd Segment Size\n",
            " Avg Bwd Segment Size\n",
            " Fwd Header Length.1\n",
            "Subflow Fwd Packets\n",
            " Subflow Fwd Bytes\n",
            "Init_Win_bytes_forward\n",
            " act_data_pkt_fwd\n",
            " min_seg_size_forward\n",
            "Idle Mean\n",
            " Idle Max\n",
            " Idle Min\n",
            " Inbound\n",
            "Destination IP_int\n",
            "UnixTimestamp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [33 34 35 45 48 52 58 59 60 61 62 63] are constant.\n",
            "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
            "  f = msb / msw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Models"
      ],
      "metadata": {
        "id": "ZETBKB5rKWKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score as roc_auc, accuracy_score as accuracy"
      ],
      "metadata": {
        "id": "46scG-TzMjMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest\n",
        "**To-Do:**\n",
        "*   Hyperparameter tuning."
      ],
      "metadata": {
        "id": "60pYh9NWQbIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build random forest classifier model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "rf = RandomForestClassifier(random_state = 42)\n",
        "\n",
        "# grid of parameters to search through when performing cross validation\n",
        "rf_params = {\n",
        "    'n_estimators' : [100, 200, 500],\n",
        "    'max_features' : [0.5, 0.75, 1.0],\n",
        "    'class_weight' : ['balanced']\n",
        "}\n",
        "\n",
        "# tests all permutations of the parameters outline in rf_params, returns the best performing model\n",
        "rf_model = GridSearchCV(estimator = rf,\n",
        "                        param_grid = rf_params,\n",
        "                        scoring = [\"accuracy\", \"f1_weighted\", \"roc_auc_ovr\"],\n",
        "                        refit = \"f1_weighted\",\n",
        "                        cv = 5,\n",
        "                        verbose = 3,\n",
        "                        return_train_score = True)"
      ],
      "metadata": {
        "id": "9a18V2MuQXB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Random Forest"
      ],
      "metadata": {
        "id": "XLFFUMQVJvo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Rfd2t83SVBfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training results\n",
        "rf_train = rf_model.predict(X_train)\n",
        "\n",
        "# displays the best model produced in training as well as its' hyperparameters and f1 score\n",
        "print(f'Training Result:\\n Best Model: {rf_model.best_estimator_}\\n Best Parameters: {rf_model.best_params_}\\n F1 Score: {rf_model.best_score_}')\n",
        "\n",
        "# evaluate the models performance and display scores\n",
        "print(f'Random Forest (TRAINING):\\n accuracy: {accuracy(y_train, rf_train):f}\\n f1 score: {f1_score(y_train, rf_train):f}\\n roc area under curve: {roc_auc(y_train, rf_train):f}')"
      ],
      "metadata": {
        "id": "VXbnWb1zK2tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Random Forest"
      ],
      "metadata": {
        "id": "_xyy5kb4JxiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on test data\n",
        "rf_test = rf_model.predict(X_test)\n",
        "\n",
        "# evaluate the models performance and display scores\n",
        "print(f'Random Forest (TESTING):\\n accuracy: {accuracy(y_test, rf_test):f}\\n f1 score: {f1_score(y_test, rf_test):e}\\n roc area under curve: {roc_auc(y_test, rf_test):f}')"
      ],
      "metadata": {
        "id": "vyU9kN8kJ1FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export Model"
      ],
      "metadata": {
        "id": "lH21zCuVPyZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uses joblib to serialise trained random forest model\n",
        "SaveSKL(model = rf_model, model_path = (dl_dir + \"/random_forest.joblib\"))"
      ],
      "metadata": {
        "id": "DdnGauFPP7TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "O4bSSnwGQfiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NN Setup\n",
        "Import and install required libraries, sets some initial values."
      ],
      "metadata": {
        "id": "236PhyQolLI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import PyTorch and confirm version\n",
        "import torch\n",
        "from torch import nn\n",
        "print(\"Using PyTorch version: {}\".format(torch.__version__))\n",
        "\n",
        "# check the availability of and set the device\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device.\")"
      ],
      "metadata": {
        "id": "cHhG7NZ9lahV",
        "outputId": "38791974-5921-4bfe-c610-9621edd0ae19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch version: 2.2.1+cu121\n",
            "\n",
            "Using cpu device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install Skorch, providing a wrapper for using PyTorch with Sklearn\n",
        "!pip install skorch\n",
        "\n",
        "# import Skorch and confirm version\n",
        "from skorch import __version__ as skorch_version\n",
        "from skorch import NeuralNetClassifier\n",
        "print(\"Using Skorch version: {}\".format(skorch_version))"
      ],
      "metadata": {
        "id": "GUSn5zCTQfGC",
        "outputId": "1dc72297-9640-4f2d-e595-e2a18f565110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: skorch in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.11.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (4.66.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (3.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Neural Network\n",
        "Currently using a Multilayer Perceptron (MLP), consider swapping to a hybrid model of a MLP and Convolutional Neural Network (CNN) later."
      ],
      "metadata": {
        "id": "AsNxj_cEDydF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_MLP(nn.Module):\n",
        "  \"\"\"Class that defines a multilayer perceptron model.\"\"\"\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    \"\"\"\n",
        "    Construct a new NN_MLP object.\n",
        "\n",
        "    Parameters:\n",
        "      input_size (int): number of inputs to the input layer.\n",
        "      hidden_size (int): number of inputs to the hidden layer(s).\n",
        "      output_size (int): number of outputs from model, equivalent to number of classes.\n",
        "    Returns:\n",
        "      : no value returned.\n",
        "    \"\"\"\n",
        "    super(NN_MLP, self).__init__()\n",
        "    # layers\n",
        "    self.h1 = nn.Linear(input_size, hidden_size)\n",
        "    self.h2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.output = nn.Linear(hidden_size, output_size)\n",
        "    # activation functions\n",
        "    self.relu = nn.ReLu()\n",
        "    self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      X (Any): features to make prediction on.\n",
        "    Returns:\n",
        "      Any: predicted value.\n",
        "    \"\"\"\n",
        "    out = self.h1(X)\n",
        "    out = self.relu(out)\n",
        "    out = self.h2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.output(out)\n",
        "    out = self.softmax(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "OFTGLP7jknbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Emissions"
      ],
      "metadata": {
        "id": "wwcoP3mZWDfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emissions = tracker.stop()\n",
        "print(f\"Emissions: {emissions * 1_000} kg CO\")"
      ],
      "metadata": {
        "id": "Xgw0UaTjWioN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}