{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "60pYh9NWQbIB",
        "O4bSSnwGQfiL"
      ],
      "authorship_tag": "ABX9TyOAqoV1WR9x71lbauF84+b4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frosk-Kristian/COMP6002-Group10-Models/blob/develop/COMP6002_Group_Project_ML_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMP6002 Computer Science Project - Group 10\n",
        "Utilising Machine Learning to detect DDoS attacks."
      ],
      "metadata": {
        "id": "vkidwNbBPly0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "zN1_GsQjkwEd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wgQiJbJiPMYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0cd1e3f-72ec-40c3-b359-a6a0687c9b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Pandas version: 1.5.3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "print(\"Using Pandas version: {}\".format(pd.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask\n",
        "\n",
        "import dask\n",
        "print(\"Using Dask version: {}\".format(dask.__version__))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxddjYl-k_DT",
        "outputId": "8773e764-acaa-4b51-cc6b-001a9ceec9b8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (2023.8.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask) (24.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask) (1.4.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask) (7.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask) (3.18.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask) (1.0.0)\n",
            "Using Dask version: 2023.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask-ml\n",
        "\n",
        "import dask_ml\n",
        "print(\"Using Dask-ML version: {}\".format(dask_ml.__version__))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF5M0nW8oVCK",
        "outputId": "925183ac-7467-432d-f691-3a19ea67bcb8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask-ml in /usr/local/lib/python3.10/dist-packages (2024.3.20)\n",
            "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (2023.8.1)\n",
            "Requirement already satisfied: distributed>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (2023.8.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dask-ml) (1.11.4)\n",
            "Requirement already satisfied: dask-glm>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (0.3.2)\n",
            "Requirement already satisfied: multipledispatch>=0.4.9 in /usr/local/lib/python3.10/dist-packages (from dask-ml) (1.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from dask-ml) (24.0)\n",
            "Requirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from dask-glm>=0.2.0->dask-ml) (2.2.1)\n",
            "Requirement already satisfied: sparse>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from dask-glm>=0.2.0->dask-ml) (0.15.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (8.1.7)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (2023.6.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (1.4.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (7.1.0)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (3.1.3)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (1.0.8)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (3.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (6.3.3)\n",
            "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (2.0.7)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask-ml) (3.0.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->dask-ml) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dask-ml) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dask-ml) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->dask-ml) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->dask-ml) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[array,dataframe]>=2.4.0->dask-ml) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask-ml) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24.2->dask-ml) (1.16.0)\n",
            "Using Dask-ML version: 2024.3.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import __version__ as skl_ver\n",
        "print(\"Using Sklearn version: {}\".format(skl_ver))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg4eRaq0fNry",
        "outputId": "796126c4-ea4d-4c3e-d5f1-ca5790703c78"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Sklearn version: 1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import & Export Trained Models"
      ],
      "metadata": {
        "id": "eEz2Qzc_kzkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions for exporting/importing models trained with Sklearn, do not attempt to use with Neural Network as the library used has its' own methods for exporting/importing\n",
        "# all functions defined will prompt the user for confirmation, to allow for skipping the functions when running the notebook\n",
        "import joblib\n",
        "print(\"Using Joblib version: {}\".format(joblib.__version__))\n",
        "\n",
        "# Save a trained model to the provided filepath\n",
        "def SaveSKL(model, model_fpath):\n",
        "  \"\"\"\n",
        "  Exports a trained model via joblib.\n",
        "\n",
        "  Parameters:\n",
        "    model (object): model to be saved.\n",
        "    model_fpath (string): file path that the model will be saved in.\n",
        "  Returns:\n",
        "    : no value returned.\n",
        "  \"\"\"\n",
        "  to_save = \"\"\n",
        "  while to_save.lower() not in ('y', 'n'):\n",
        "    to_save = input(\"Do you wish to save the trained model? (y/n)\\n\")\n",
        "    if to_save.lower() in 'y':\n",
        "      print(\"Saving model to: {}\".format(model_fpath))\n",
        "      try:\n",
        "        joblib.dump(model, model_fpath)\n",
        "        print(\"SUCCESS: Model saved to {}\".format(model_fpath))\n",
        "      except:\n",
        "        print(\"ERROR: An unknown error has occured when calling joblib.dump()!\")\n",
        "    else:\n",
        "      if to_save.lower() in 'n':\n",
        "        print(\"Did not save model.\")\n",
        "      else:\n",
        "        print(\"Please only enter \\'y\\' to save model or \\'n\\' to skip saving.\")\n",
        "\n",
        "# Load a trained model from the provided filepath\n",
        "def LoadSKL(model_fpath):\n",
        "  \"\"\"\n",
        "  Import a trained model via joblib.\n",
        "\n",
        "  Parameters:\n",
        "    model_fpath (string): file path to the stored model.\n",
        "  Returns:\n",
        "    object: if a model is found and loaded correctly, returns an object.\n",
        "    None: if no matching file is found or an error occurs during loading, returns None.\n",
        "  \"\"\"\n",
        "  to_load = \"\"\n",
        "  while to_load.lower() not in ('y', 'n'):\n",
        "    to_load = input(\"Do you wish to import a trained model? {y/n)\\n\")\n",
        "    if to_load.lower() in 'y':\n",
        "      model = None\n",
        "      print(\"Attempting to import model from: {}\".format(model_fpath))\n",
        "      try:\n",
        "        model = joblib.load(model_fpath)\n",
        "        print(\"SUCCESS: Model successfully imported.\")\n",
        "      except FileNotFoundError:\n",
        "        print(\"ERROR: The file \\'{}\\' does not exist!\".format(model_fpath))\n",
        "        model = None\n",
        "      except:\n",
        "        print(\"ERROR: An unknown error has occured when calling joblib.load()!\")\n",
        "        model = None\n",
        "      finally:\n",
        "        return model\n",
        "    else:\n",
        "      if to_load.lower() in 'n':\n",
        "        print(\"Did not import model.\")\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Please only enter \\'y\\' to import model or \\'n\\' to skip importing.\")\n",
        "\n",
        "# To-do: write function that exports model parameters, evaluation metrics, etc."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otJrdxCQXWv5",
        "outputId": "344c184d-c6ec-4bb9-9ad4-edc418e9a12f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Joblib version: 1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data\n",
        "Checks current working directory for datasets, if datasets are missing downloads a [.zip archive mirror of the CiCDDoS2019 hosted on Kaggle](https://www.kaggle.com/datasets/kristianfrossos/cicddos2019/data).\n",
        "\n",
        "**NOTE:** the first part of this section is specific to Google Colab, and will not work outside of it. Advise writing an alternative later for local use (relevant when training Neural Network for speed and when usage limits get in the way).\n",
        "\n",
        "## Reference\n",
        "Iman Sharafaldin, Arash Habibi Lashkari, Saqib Hakak, and Ali A. Ghorbani, \"Developing Realistic Distributed Denial of Service (DDoS) Attack Dataset and Taxonomy\", IEEE 53rd International Carnahan Conference on Security Technology, Chennai, India, 2019.\n"
      ],
      "metadata": {
        "id": "gSVADpSUPvxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "rVY0RkBfOV1C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets up dataset directory\n",
        "import os"
      ],
      "metadata": {
        "id": "YRTgsphl2JqO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mounts google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# directory that the dataset will be downloaded to\n",
        "dl_dir = os.getcwd() + r'/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data'\n",
        "\n",
        "if os.path.exists(dl_dir):\n",
        "  print(\"Directory {} already exists.\\n\".format(dl_dir))\n",
        "else:\n",
        "  os.mkdir(dl_dir)\n",
        "  print(\"Successfully created the directory {}\".format(dl_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUdWg_050_Ro",
        "outputId": "0f60099b-ce8e-47c2-8fde-c5c38b2f0332"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Directory /content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data already exists.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sets up kaggle environment variables (needed to access API)\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "\n",
        "# checks if kaggle key and username have been provided as secrets and sets environment variables appropriately\n",
        "# if not found, attempts to use kaggle.json\n",
        "try:\n",
        "  os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "  os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "  print(\"Using KAGGLE_KEY and KAGGLE_USERNAME defined in secrets.\")\n",
        "except (userdata.SecretNotFoundError, userdata.NotebookAccessError):\n",
        "  print(\"WARN: One or more secret(s) missing or inaccessible.\\n\")\n",
        "  if os.path.isfile('~/.kaggle/kaggle.json'):\n",
        "    print(\"Using existing kaggle.json\")\n",
        "  else:\n",
        "    print(\"Please upload kaggle.json\")\n",
        "    files.upload()\n",
        "\n",
        "    if os.path.isfile(os.getcwd() + '/content/kaggle.json'):\n",
        "      !rm -r ~/.kaggle\n",
        "      !mkdir ~/.kaggle\n",
        "      !mv ./kaggle.json ~/.kaggle/\n",
        "      !chmod 600 ~/.kaggle/kaggle.json\n",
        "    else:\n",
        "      print(\"\\'kaggle.json\\' not uploaded.\")\n",
        "      raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcOT1v4TO4rm",
        "outputId": "35118d7a-fedb-479b-e3d3-9f3d4e717307"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using KAGGLE_KEY and KAGGLE_USERNAME defined in secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.getcwd() + '/dataset'\n",
        "\n",
        "# checks if .zip archive containing dataset already exists in google drive and downloads it if necessary\n",
        "if os.path.isfile(dl_dir + '/cicddos2019.zip'):\n",
        "  print(\"Dataset already present.\")\n",
        "else:\n",
        "  print(\"Downloading zipped dataset to {}\".format(dl_dir))\n",
        "  !kaggle datasets download kristianfrossos/cicddos2019 -p {dl_dir.replace(' ', '\\ ')}\n",
        "\n",
        "# creates the content/dataset directory if it doesn't already exist\n",
        "if os.path.exists(data_dir):\n",
        "  print(\"Directory {} already exists.\\n\".format(data_dir))\n",
        "else:\n",
        "  print(\"Created directory: {}\\n\".format(data_dir))\n",
        "  os.mkdir(data_dir)\n",
        "\n",
        "# extracts contents of .zip archive to content/dataset if directory is not empty\n",
        "if not os.listdir(data_dir):\n",
        "  print(\"Empty directory, extracting dataset.\")\n",
        "  # unzips .zip archive\n",
        "  !unzip {dl_dir.replace(' ', '\\ ') + '/cicddos2019.zip'} -d {data_dir}\n",
        "else:\n",
        "  print(\"Non-empty directory, skipping download\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUQvxpnNMnRQ",
        "outputId": "f925b7f9-7905-4770-bd32-93ec6c7f5969"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already present.\n",
            "Directory /content/dataset already exists.\n",
            "\n",
            "Non-empty directory, skipping download\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialises empty list\n",
        "csv_list = []\n",
        "\n",
        "# iterates through all subdirectories of /content/dataset and appends the filepath of each .csv to csv_list\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "  for f in files:\n",
        "    if f.endswith(\".csv\"):\n",
        "      csv_list.append(os.path.join(root, f))\n",
        "\n",
        "# if .csv files were found, displays number of files and prints each path\n",
        "if not csv_list:\n",
        "  print(\"No .csv files found!\")\n",
        "else:\n",
        "  print(\"{} .csv files found.\".format(len(csv_list)))\n",
        "  for csv in csv_list:\n",
        "    print(csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMw9ryJSbiPL",
        "outputId": "e9a326cc-6d1f-493e-defe-7f9bec0f9221"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18 .csv files found.\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_SSDP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_SNMP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_NTP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_DNS.csv\n",
            "/content/dataset/CSV-01-12/01-12/Syn.csv\n",
            "/content/dataset/CSV-01-12/01-12/TFTP.csv\n",
            "/content/dataset/CSV-01-12/01-12/UDPLag.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_NetBIOS.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_LDAP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_MSSQL.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_UDP.csv\n",
            "/content/dataset/CSV-03-11/03-11/UDP.csv\n",
            "/content/dataset/CSV-03-11/03-11/NetBIOS.csv\n",
            "/content/dataset/CSV-03-11/03-11/Syn.csv\n",
            "/content/dataset/CSV-03-11/03-11/UDPLag.csv\n",
            "/content/dataset/CSV-03-11/03-11/MSSQL.csv\n",
            "/content/dataset/CSV-03-11/03-11/LDAP.csv\n",
            "/content/dataset/CSV-03-11/03-11/Portmap.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dask import dataframe as df\n",
        "\n",
        "dask_df = df.read_csv(csv_list, dtype = {'SimillarHTTP': 'object'})"
      ],
      "metadata": {
        "id": "cAAImz65is08"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DfAnalyse(df_):\n",
        "  \"\"\"\n",
        "  Function that prints the index, name and dtype of every feature in a dataframe.\n",
        "  Prints two separate columns equal to half the dataframes length.\n",
        "\n",
        "  Parameters:\n",
        "    df_ (dataframe): dataframe to analyse.\n",
        "  Returns:\n",
        "    : no return value.\n",
        "  \"\"\"\n",
        "  cols = dask_df.columns.values.tolist() # list of column names\n",
        "  types = dask_df.dtypes.tolist() # list of column dtypes\n",
        "\n",
        "  if len(cols) == len(types):\n",
        "    print(\"{} columns.\".format(len(cols)))\n",
        "    half_len = len(cols) // 2 # half length, round down\n",
        "    for i in range(0, half_len):\n",
        "      val1 = f\"{i}: \\'{cols[i]}\\' ({types[i]})\"\n",
        "      val2 = f\"{half_len+i}: \\'{cols[half_len+i]}\\' ({types[half_len+i]})\"\n",
        "      pad = 50 - len(val1) # padding between columns\n",
        "      print(f\"{val1}{' ' * pad}{val2}\")\n",
        "\n",
        "    # if length is odd number, prints last value\n",
        "    if (len(cols) % 2) != 0:\n",
        "      print(f\"{len(cols)}: \\'{cols[-1]}\\' ({types[-1]})\")\n",
        "  else:\n",
        "    print(\"WARNING: len(cols) ({}) does not equal len(types) ({})!\".format(len(cols), len(types)))"
      ],
      "metadata": {
        "id": "2q9FOCrNMWRI"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# describes all object columns\n",
        "dask_df.describe(include=['O'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "5xOzCY8Zm-mh",
        "outputId": "c808fa65-7061-43a7-e0ab-c5f8a29b77cd"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dask DataFrame Structure:\n",
              "              Flow ID  Source IP  Destination IP  Timestamp SimillarHTTP   Label\n",
              "npartitions=1                                                                   \n",
              "               object     object          object     object       object  object\n",
              "                  ...        ...             ...        ...          ...     ...\n",
              "Dask Name: describe, 68 graph layers"
            ],
            "text/html": [
              "<div><strong>Dask DataFrame Structure:</strong></div>\n",
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Flow ID</th>\n",
              "      <th>Source IP</th>\n",
              "      <th>Destination IP</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>SimillarHTTP</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>npartitions=1</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "<div>Dask Name: describe, 68 graph layers</div>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "**To-Do:**\n",
        "*   Substitute missing values with the mean of their respective columns.\n",
        "    *  Not feasible with size of dataset.\n",
        "    *  Break dataset into several batches and preprocess per batch?\n",
        "*   Encode categorical columns.\n",
        "*   Normalise dataset.\n",
        "\n",
        "## Dropped Features\n",
        "*   Unnamed: 0: unknown feature.\n",
        "*   Flow Id: constructed from Source Ip, Destination Ip, Source Port, Destination Port and Protocol."
      ],
      "metadata": {
        "id": "2IH9h-H8QULd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset into separatate training and testing datasets\n",
        "from dask_ml.model_selection import train_test_split\n",
        "import ipaddress\n",
        "\n",
        "# drops columns\n",
        "to_drop = ['Unnamed: 0', 'Flow ID']\n",
        "dask_df = dask_df.drop(columns = to_drop)\n",
        "\n",
        "# drop columns with at least roughly 50% missing values\n",
        "# as it takes a very long time to compute length, uses hardcoded threshold based on a previously computed length (70427637 rows)\n",
        "dask_df = dask_df.dropna(axis = 1, thresh = int(35213818))\n",
        "\n",
        "# drop duplicate rows\n",
        "dask_df.drop_duplicates()\n",
        "\n",
        "# convert ip addresses to integers\n",
        "dask_df['Source IP_int'] = dask_df.apply(lambda x: int (ipaddress.IPv4Address(x[' Source IP'])), meta=('Source IP_int', 'int'), axis=1)\n",
        "dask_df['Destination IP_int'] = dask_df.apply(lambda x: int (ipaddress.IPv4Address(x[' Destination IP'])), meta=('Destination IP_int', 'int'), axis=1)\n",
        "# drop ip addresses\n",
        "dask_df = dask_df.drop(columns = [' Source IP', ' Destination IP'])\n",
        "\n",
        "# convert timestamps to datetime format\n",
        "dask_df[' Timestamp'] = df.to_datetime(dask_df[' Timestamp'], format = '%Y-%m-%d %H:%M:%S.%f', meta = (' Timestamp', 'datetime64[ns]'))\n",
        "\n",
        "# splits across x and y axis\n",
        "X = dask_df.drop(columns = ' Label')\n",
        "y = dask_df[' Label']\n",
        "\n",
        "# splits 20% of training data into testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, shuffle = True)"
      ],
      "metadata": {
        "id": "NmI7AY1fQWmp"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DfAnalyse(dask_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw1YVz6OBWwF",
        "outputId": "19fcf5a2-3631-49f4-bf73-c35836a4aa3a"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88 columns.\n",
            "0: 'Unnamed: 0' (int64)                           44: ' Bwd Packets/s' (float64)\n",
            "1: 'Flow ID' (object)                             45: ' Min Packet Length' (float64)\n",
            "2: ' Source IP' (object)                          46: ' Max Packet Length' (float64)\n",
            "3: ' Source Port' (int64)                         47: ' Packet Length Mean' (float64)\n",
            "4: ' Destination IP' (object)                     48: ' Packet Length Std' (float64)\n",
            "5: ' Destination Port' (int64)                    49: ' Packet Length Variance' (float64)\n",
            "6: ' Protocol' (int64)                            50: 'FIN Flag Count' (int64)\n",
            "7: ' Timestamp' (object)                          51: ' SYN Flag Count' (int64)\n",
            "8: ' Flow Duration' (int64)                       52: ' RST Flag Count' (int64)\n",
            "9: ' Total Fwd Packets' (int64)                   53: ' PSH Flag Count' (int64)\n",
            "10: ' Total Backward Packets' (int64)             54: ' ACK Flag Count' (int64)\n",
            "11: 'Total Length of Fwd Packets' (float64)       55: ' URG Flag Count' (int64)\n",
            "12: ' Total Length of Bwd Packets' (float64)      56: ' CWE Flag Count' (int64)\n",
            "13: ' Fwd Packet Length Max' (float64)            57: ' ECE Flag Count' (int64)\n",
            "14: ' Fwd Packet Length Min' (float64)            58: ' Down/Up Ratio' (float64)\n",
            "15: ' Fwd Packet Length Mean' (float64)           59: ' Average Packet Size' (float64)\n",
            "16: ' Fwd Packet Length Std' (float64)            60: ' Avg Fwd Segment Size' (float64)\n",
            "17: 'Bwd Packet Length Max' (float64)             61: ' Avg Bwd Segment Size' (float64)\n",
            "18: ' Bwd Packet Length Min' (float64)            62: ' Fwd Header Length.1' (int64)\n",
            "19: ' Bwd Packet Length Mean' (float64)           63: 'Fwd Avg Bytes/Bulk' (int64)\n",
            "20: ' Bwd Packet Length Std' (float64)            64: ' Fwd Avg Packets/Bulk' (int64)\n",
            "21: 'Flow Bytes/s' (float64)                      65: ' Fwd Avg Bulk Rate' (int64)\n",
            "22: ' Flow Packets/s' (float64)                   66: ' Bwd Avg Bytes/Bulk' (int64)\n",
            "23: ' Flow IAT Mean' (float64)                    67: ' Bwd Avg Packets/Bulk' (int64)\n",
            "24: ' Flow IAT Std' (float64)                     68: 'Bwd Avg Bulk Rate' (int64)\n",
            "25: ' Flow IAT Max' (float64)                     69: 'Subflow Fwd Packets' (int64)\n",
            "26: ' Flow IAT Min' (float64)                     70: ' Subflow Fwd Bytes' (int64)\n",
            "27: 'Fwd IAT Total' (float64)                     71: ' Subflow Bwd Packets' (int64)\n",
            "28: ' Fwd IAT Mean' (float64)                     72: ' Subflow Bwd Bytes' (int64)\n",
            "29: ' Fwd IAT Std' (float64)                      73: 'Init_Win_bytes_forward' (int64)\n",
            "30: ' Fwd IAT Max' (float64)                      74: ' Init_Win_bytes_backward' (int64)\n",
            "31: ' Fwd IAT Min' (float64)                      75: ' act_data_pkt_fwd' (int64)\n",
            "32: 'Bwd IAT Total' (float64)                     76: ' min_seg_size_forward' (int64)\n",
            "33: ' Bwd IAT Mean' (float64)                     77: 'Active Mean' (float64)\n",
            "34: ' Bwd IAT Std' (float64)                      78: ' Active Std' (float64)\n",
            "35: ' Bwd IAT Max' (float64)                      79: ' Active Max' (float64)\n",
            "36: ' Bwd IAT Min' (float64)                      80: ' Active Min' (float64)\n",
            "37: 'Fwd PSH Flags' (int64)                       81: 'Idle Mean' (float64)\n",
            "38: ' Bwd PSH Flags' (int64)                      82: ' Idle Std' (float64)\n",
            "39: ' Fwd URG Flags' (int64)                      83: ' Idle Max' (float64)\n",
            "40: ' Bwd URG Flags' (int64)                      84: ' Idle Min' (float64)\n",
            "41: ' Fwd Header Length' (int64)                  85: 'SimillarHTTP' (object)\n",
            "42: ' Bwd Header Length' (int64)                  86: ' Inbound' (int64)\n",
            "43: 'Fwd Packets/s' (float64)                     87: ' Label' (object)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "60pYh9NWQbIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build random forest classifier model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "rf = RandomForestClassifier(random_state = 42)\n",
        "\n",
        "# grid of parameters to search through when performing cross validation\n",
        "rf_params = {\n",
        "    'class_weight' : ['balanced']\n",
        "}\n",
        "\n",
        "# tests all permutations of the parameters outline in rf_params, returns the best performing model\n",
        "rf_model = GridSearchCV(estimator = rf,\n",
        "                        param_grid = rf_params,\n",
        "                        scoring = [\"accuracy\", \"f1_weighted\", \"roc_auc_ovr\"],\n",
        "                        refit = \"f1_weighted\",\n",
        "                        cv = 5,\n",
        "                        verbose = 3,\n",
        "                        return_train_score = True)\n",
        "\n",
        "# rf_model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "9a18V2MuQXB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "O4bSSnwGQfiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN Setup\n",
        "Import and install required libraries, sets some initial values."
      ],
      "metadata": {
        "id": "236PhyQolLI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import PyTorch and confirm version\n",
        "import torch\n",
        "from torch import nn\n",
        "print(\"Using PyTorch version: {}\".format(torch.__version__))\n",
        "\n",
        "# check the availability of and set the device\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(\"Using {} device.\".format(device))"
      ],
      "metadata": {
        "id": "cHhG7NZ9lahV",
        "outputId": "38791974-5921-4bfe-c610-9621edd0ae19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch version: 2.2.1+cu121\n",
            "\n",
            "Using cpu device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install Skorch, providing a wrapper for using PyTorch with Sklearn\n",
        "!pip install skorch\n",
        "\n",
        "# import Skorch and confirm version\n",
        "from skorch import __version__ as skorch_version\n",
        "from skorch import NeuralNetClassifier\n",
        "print(\"Using Skorch version: {}\".format(skorch_version))"
      ],
      "metadata": {
        "id": "GUSn5zCTQfGC",
        "outputId": "1dc72297-9640-4f2d-e595-e2a18f565110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: skorch in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.11.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (4.66.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (3.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Neural Network\n",
        "Currently using a Multilayer Perceptron (MLP), consider swapping to a hybrid model of a MLP and Convolutional Neural Network (CNN) later."
      ],
      "metadata": {
        "id": "AsNxj_cEDydF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_MLP(nn.Module):\n",
        "  \"\"\"Class that defines a multilayer perceptron model.\"\"\"\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    \"\"\"\n",
        "    Construct a new NN_MLP object.\n",
        "\n",
        "    Parameters:\n",
        "      input_size (int): number of inputs to the input layer.\n",
        "      hidden_size (int): number of inputs to the hidden layer(s).\n",
        "      output_size (int): number of outputs from the output layer, typically 1.\n",
        "    Returns:\n",
        "      : no value returned.\n",
        "    \"\"\"\n",
        "    super(NN_MLP, self).__init__()\n",
        "    # layers\n",
        "    self.h1 = nn.Linear(input_size, hidden_size)\n",
        "    self.h2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.output = nn.Linear(hidden_size, output_size)\n",
        "    # activation functions\n",
        "    self.relu = nn.ReLu()\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      X (Any): features to make prediction on.\n",
        "    Returns:\n",
        "      Any: predicted value.\n",
        "    \"\"\"\n",
        "    out = self.h1(X)\n",
        "    out = self.relu(out)\n",
        "    out = self.h2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.output(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "OFTGLP7jknbq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}