{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "60pYh9NWQbIB",
        "O4bSSnwGQfiL"
      ],
      "authorship_tag": "ABX9TyPTDBBZk3N/xnB5DPYzUYoJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frosk-Kristian/COMP6002-Group10-Models/blob/develop/COMP6002_Group_Project_ML_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMP6002 Computer Science Project - Group 10\n",
        "Utilising Machine Learning to detect DDoS attacks."
      ],
      "metadata": {
        "id": "vkidwNbBPly0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "zN1_GsQjkwEd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wgQiJbJiPMYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a07f51-6bc2-4002-85ff-55685995ddab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Pandas version: 2.0.3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "print(\"Using Pandas version: {}\".format(pd.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import __version__ as skl_ver\n",
        "print(\"Using Sklearn version: {}\".format(skl_ver))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg4eRaq0fNry",
        "outputId": "a30b755b-2a73-4961-a1a4-119d57329477"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Sklearn version: 1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import & Export Trained Models"
      ],
      "metadata": {
        "id": "eEz2Qzc_kzkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions for exporting/importing models trained with Sklearn, do not attempt to use with Neural Network as the library used has its' own methods for exporting/importing\n",
        "# all functions defined will prompt the user for confirmation, to allow for skipping the functions when running the notebook\n",
        "import joblib\n",
        "print(\"Using Joblib version: {}\".format(joblib.__version__))\n",
        "\n",
        "# Save a trained model to the provided filepath\n",
        "def SaveSKL(model, model_fpath):\n",
        "  \"\"\"\n",
        "  Exports a trained model via joblib.\n",
        "\n",
        "  Parameters:\n",
        "    model (object): model to be saved.\n",
        "    model_fpath (string): file path that the model will be saved in.\n",
        "  Returns:\n",
        "    : no value returned.\n",
        "  \"\"\"\n",
        "  to_save = \"\"\n",
        "  while to_save.lower() not in ('y', 'n'):\n",
        "    to_save = input(\"Do you wish to save the trained model? (y/n)\\n\")\n",
        "    if to_save.lower() in 'y':\n",
        "      print(\"Saving model to: {}\".format(model_fpath))\n",
        "      try:\n",
        "        joblib.dump(model, model_fpath)\n",
        "        print(\"SUCCESS: Model saved to {}\".format(model_fpath))\n",
        "      except:\n",
        "        print(\"ERROR: An unknown error has occured when calling joblib.dump()!\")\n",
        "    else:\n",
        "      if to_save.lower() in 'n':\n",
        "        print(\"Did not save model.\")\n",
        "      else:\n",
        "        print(\"Please only enter \\'y\\' to save model or \\'n\\' to skip saving.\")\n",
        "\n",
        "# Load a trained model from the provided filepath\n",
        "def LoadSKL(model_fpath):\n",
        "  \"\"\"\n",
        "  Import a trained model via joblib.\n",
        "\n",
        "  Parameters:\n",
        "    model_fpath (string): file path to the stored model.\n",
        "  Returns:\n",
        "    object: if a model is found and loaded correctly, returns an object.\n",
        "    None: if no matching file is found or an error occurs during loading, returns None.\n",
        "  \"\"\"\n",
        "  to_load = \"\"\n",
        "  while to_load.lower() not in ('y', 'n'):\n",
        "    to_load = input(\"Do you wish to import a trained model? {y/n)\\n\")\n",
        "    if to_load.lower() in 'y':\n",
        "      model = None\n",
        "      print(\"Attempting to import model from: {}\".format(model_fpath))\n",
        "      try:\n",
        "        model = joblib.load(model_fpath)\n",
        "        print(\"SUCCESS: Model successfully imported.\")\n",
        "      except FileNotFoundError:\n",
        "        print(\"ERROR: The file \\'{}\\' does not exist!\".format(model_fpath))\n",
        "        model = None\n",
        "      except:\n",
        "        print(\"ERROR: An unknown error has occured when calling joblib.load()!\")\n",
        "        model = None\n",
        "      finally:\n",
        "        return model\n",
        "    else:\n",
        "      if to_load.lower() in 'n':\n",
        "        print(\"Did not import model.\")\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Please only enter \\'y\\' to import model or \\'n\\' to skip importing.\")\n",
        "\n",
        "# To-do: write function that exports model parameters, evaluation metrics, etc."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otJrdxCQXWv5",
        "outputId": "c7f59906-782a-49d8-ab34-826c6b857faa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Joblib version: 1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data\n",
        "Checks current working directory for datasets, if datasets are missing downloads a [.zip archive mirror of the CiCDDoS2019 hosted on Kaggle](https://www.kaggle.com/datasets/kristianfrossos/cicddos2019/data).\n",
        "\n",
        "**NOTE:** the first part of this section is specific to Google Colab, and will not work outside of it. Advise writing an alternative later for local use (relevant when training Neural Network for speed and when usage limits get in the way).\n",
        "\n",
        "## Reference\n",
        "Iman Sharafaldin, Arash Habibi Lashkari, Saqib Hakak, and Ali A. Ghorbani, \"Developing Realistic Distributed Denial of Service (DDoS) Attack Dataset and Taxonomy\", IEEE 53rd International Carnahan Conference on Security Technology, Chennai, India, 2019.\n"
      ],
      "metadata": {
        "id": "gSVADpSUPvxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "rVY0RkBfOV1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb042139-d422-4de7-ff4b-148fa00fde59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/79.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets up dataset directory\n",
        "import os"
      ],
      "metadata": {
        "id": "YRTgsphl2JqO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mounts google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# directory that the dataset will be downloaded to\n",
        "dl_dir = os.getcwd() + r'/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data'\n",
        "\n",
        "if os.path.exists(dl_dir):\n",
        "  print(\"Directory {} already exists.\\n\".format(dl_dir))\n",
        "else:\n",
        "  os.mkdir(dl_dir)\n",
        "  print(\"Successfully created the directory {}\".format(dl_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUdWg_050_Ro",
        "outputId": "eb7f6a30-42ea-427e-bef4-83513d61ae5c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Directory /content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data already exists.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sets up kaggle environment variables (needed to access API)\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "\n",
        "# checks if kaggle key and username have been provided as secrets and sets environment variables appropriately\n",
        "# if not found, attempts to use kaggle.json\n",
        "try:\n",
        "  os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "  os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "  print(\"Using KAGGLE_KEY and KAGGLE_USERNAME defined in secrets.\")\n",
        "except (userdata.SecretNotFoundError, userdata.NotebookAccessError):\n",
        "  print(\"WARN: One or more secret(s) missing or inaccessible.\\n\")\n",
        "  if os.path.isfile('~/.kaggle/kaggle.json'):\n",
        "    print(\"Using existing kaggle.json\")\n",
        "  else:\n",
        "    print(\"Please upload kaggle.json\")\n",
        "    files.upload()\n",
        "\n",
        "    if os.path.isfile(os.getcwd() + '/content/kaggle.json'):\n",
        "      !rm -r ~/.kaggle\n",
        "      !mkdir ~/.kaggle\n",
        "      !mv ./kaggle.json ~/.kaggle/\n",
        "      !chmod 600 ~/.kaggle/kaggle.json\n",
        "    else:\n",
        "      print(\"\\'kaggle.json\\' not uploaded.\")\n",
        "      raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcOT1v4TO4rm",
        "outputId": "71dc1ebf-528d-4d81-a562-fdffcf608141"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using KAGGLE_KEY and KAGGLE_USERNAME defined in secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.getcwd() + '/dataset'\n",
        "\n",
        "# checks if .zip archive containing dataset already exists in google drive and downloads it if necessary\n",
        "if os.path.isfile(dl_dir + '/cicddos2019.zip'):\n",
        "  print(\"Dataset already present.\")\n",
        "else:\n",
        "  print(\"Downloading zipped dataset to {}\".format(dl_dir))\n",
        "  !kaggle datasets download kristianfrossos/cicddos2019 -p {dl_dir.replace(' ', '\\ ')}\n",
        "\n",
        "# creates the content/dataset directory if it doesn't already exist\n",
        "if os.path.exists(data_dir):\n",
        "  print(\"Directory {} already exists.\\n\".format(data_dir))\n",
        "else:\n",
        "  print(\"Created directory: {}\\n\".format(data_dir))\n",
        "  os.mkdir(data_dir)\n",
        "\n",
        "# extracts contents of .zip archive to content/dataset if directory is not empty\n",
        "if not os.listdir(data_dir):\n",
        "  print(\"Empty directory, extracting dataset.\")\n",
        "  # unzips .zip archive\n",
        "  !unzip {dl_dir.replace(' ', '\\ ') + '/cicddos2019.zip'} -d {data_dir}\n",
        "else:\n",
        "  print(\"Non-empty directory, skipping download\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUQvxpnNMnRQ",
        "outputId": "84fcb6fe-317a-4292-ca38-ba3a68cc3981"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already present.\n",
            "Created directory: /content/dataset\n",
            "\n",
            "Empty directory, extracting dataset.\n",
            "Archive:  /content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data/cicddos2019.zip\n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_DNS.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_LDAP.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_MSSQL.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_NTP.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_NetBIOS.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_SNMP.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_SSDP.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_UDP.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/Syn.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/TFTP.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/UDPLag.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/LDAP.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/MSSQL.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/NetBIOS.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/Portmap.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/Syn.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/UDP.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/UDPLag.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialises empty list\n",
        "csv_list = []\n",
        "\n",
        "# iterates through all subdirectories of /content/dataset and appends the filepath of each .csv to csv_list\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "  for f in files:\n",
        "    if f.endswith(\".csv\"):\n",
        "      csv_list.append(os.path.join(root, f))\n",
        "\n",
        "# if .csv files were found, displays number of files and prints each path\n",
        "if not csv_list:\n",
        "  print(\"No .csv files found!\")\n",
        "else:\n",
        "  print(\"{} .csv files found.\".format(len(csv_list)))\n",
        "  for csv in csv_list:\n",
        "    print(csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMw9ryJSbiPL",
        "outputId": "8633a5c4-8a57-4c23-cd2a-2304248afb27"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18 .csv files found.\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_MSSQL.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_DNS.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_SNMP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_LDAP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_NetBIOS.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_NTP.csv\n",
            "/content/dataset/CSV-01-12/01-12/TFTP.csv\n",
            "/content/dataset/CSV-01-12/01-12/Syn.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_SSDP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_UDP.csv\n",
            "/content/dataset/CSV-01-12/01-12/UDPLag.csv\n",
            "/content/dataset/CSV-03-11/03-11/NetBIOS.csv\n",
            "/content/dataset/CSV-03-11/03-11/Portmap.csv\n",
            "/content/dataset/CSV-03-11/03-11/UDP.csv\n",
            "/content/dataset/CSV-03-11/03-11/Syn.csv\n",
            "/content/dataset/CSV-03-11/03-11/LDAP.csv\n",
            "/content/dataset/CSV-03-11/03-11/UDPLag.csv\n",
            "/content/dataset/CSV-03-11/03-11/MSSQL.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddos_df = pd.DataFrame()\n",
        "benign_df = pd.DataFrame()\n",
        "\n",
        "num_samples = 100\n",
        "\n",
        "for csv in csv_list:\n",
        "  data_iter = pd.read_csv(csv, chunksize=2000)\n",
        "\n",
        "  ddos_desired = ddos_df.size + num_samples\n",
        "  benign_desired = benign_df.size + num_samples\n",
        "  print(\"Goal:\\n {} DDoS & {} Benign\".format(ddos_desired, benign_desired))\n",
        "\n",
        "  for chunk in data_iter:\n",
        "    ddos_rows = chunk[chunk[' Label'].str.lower() != 'benign']\n",
        "    benign_rows = chunk[chunk[' Label'].str.lower() == 'benign']\n",
        "\n",
        "    sample_size = min(len(ddos_rows), len(benign_rows)) // 100\n",
        "    ddos_sample = ddos_rows.sample(n=sample_size, random_state=42)\n",
        "    benign_sample = benign_rows.sample(n=sample_size, random_state=42)\n",
        "\n",
        "    ddos_df = pd.concat([ddos_df, ddos_sample], ignore_index=True)\n",
        "    benign_df = pd.concat([benign_df, benign_sample], ignore_index=True)\n",
        "\n",
        "    if ddos_df.size >= ddos_desired and benign_df.size >= benign_desired:\n",
        "      print(\"Goal reached\")\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14YJSfXt7F0Z",
        "outputId": "5e885b05-2715-4a9a-d5c1-944c2556aaf0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Goal:\n",
            " 100 DDoS & 100 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 628 DDoS & 628 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 980 DDoS & 980 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 1244 DDoS & 1244 Benign\n",
            "Goal:\n",
            " 1244 DDoS & 1244 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 1508 DDoS & 1508 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 1772 DDoS & 1772 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 2388 DDoS & 2388 Benign\n",
            "Goal:\n",
            " 2388 DDoS & 2388 Benign\n",
            "Goal:\n",
            " 2388 DDoS & 2388 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 2564 DDoS & 2564 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 2828 DDoS & 2828 Benign\n",
            "Goal:\n",
            " 2828 DDoS & 2828 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 3708 DDoS & 3708 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 4236 DDoS & 4236 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 4764 DDoS & 4764 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 5556 DDoS & 5556 Benign\n",
            "Goal reached\n",
            "Goal:\n",
            " 5732 DDoS & 5732 Benign\n",
            "Goal reached\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenates ddos and benign dataframes into one subset\n",
        "subset = pd.concat([ddos_df, benign_df], ignore_index=True)\n",
        "\n",
        "# class weights\n",
        "weights = subset.value_counts(' Label', normalize=True)\n",
        "\n",
        "# prints classes and their weights\n",
        "print(\"     Class      |     Weight\")\n",
        "for index in weights.index:\n",
        "  print(f'{index:<15} | {(weights[index] * 100.0):n}%')\n",
        "\n",
        "# prints total (for error checking)\n",
        "print(f'Total: {sum(weights.values) * 100.0}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwKDB0qpF4Xz",
        "outputId": "0b516a9c-dde6-4b99-ba5b-debbc0e7c9d2"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Class      |     Weight\n",
            "BENIGN          | 50%\n",
            "Portmap         | 7.46269%\n",
            "NetBIOS         | 6.71642%\n",
            "MSSQL           | 5.97015%\n",
            "TFTP            | 5.22388%\n",
            "DrDoS_MSSQL     | 4.47761%\n",
            "Syn             | 4.47761%\n",
            "DrDoS_DNS       | 2.98507%\n",
            "DrDoS_NTP       | 2.23881%\n",
            "DrDoS_NetBIOS   | 2.23881%\n",
            "DrDoS_SNMP      | 2.23881%\n",
            "DrDoS_UDP       | 1.49254%\n",
            "UDP             | 1.49254%\n",
            "UDP-lag         | 1.49254%\n",
            "LDAP            | 0.746269%\n",
            "WebDDoS         | 0.746269%\n",
            "Total: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "**To-Do:**\n",
        "*   Substitute missing values with the mean of their respective columns.\n",
        "    *  Not feasible with size of dataset.\n",
        "    *  Break dataset into several batches and preprocess per batch?\n",
        "*   Encode categorical columns.\n",
        "*   Normalise dataset.\n",
        "\n",
        "## Dropped Features\n",
        "*   Unnamed: 0: unknown feature.\n",
        "*   Flow Id: constructed from Source Ip, Destination Ip, Source Port, Destination Port and Protocol."
      ],
      "metadata": {
        "id": "2IH9h-H8QULd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import ipaddress\n",
        "\n",
        "# drops irrelevant columns\n",
        "subset.drop(columns = ['Unnamed: 0', 'Flow ID'],\n",
        "            inplace = True)\n",
        "\n",
        "# drop columns with at least 50% missing values\n",
        "subset.dropna(axis = 1,\n",
        "              thresh = int(0.5 * subset.shape[0]),\n",
        "              inplace = True)\n",
        "\n",
        "# drop duplicate rows\n",
        "subset.drop_duplicates(inplace = True)\n",
        "\n",
        "# converts source and destination IP addresses to useable integer values\n",
        "subset['Source IP_int'] = subset.apply(lambda x: int (ipaddress.IPv4Address(x[' Source IP'])), axis=1)\n",
        "subset['Destination IP_int'] = subset.apply(lambda x: int (ipaddress.IPv4Address(x[' Destination IP'])), axis=1)\n",
        "\n",
        "# converts date and time values to unix timestamps\n",
        "subset['UnixTimestamp'] = subset.apply(lambda x: (pd.to_datetime(x[' Timestamp']).timestamp()), axis=1)\n",
        "\n",
        "# drops original columns\n",
        "subset.drop(columns = [' Source IP', ' Destination IP', ' Timestamp'],\n",
        "            inplace = True)\n",
        "\n",
        "# save processed subset to .csv\n",
        "subset.to_csv((dl_dir + '/COMP6002_Processed_Subset.csv'), index = False)"
      ],
      "metadata": {
        "id": "Ncl7SwVoSldZ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "60pYh9NWQbIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build random forest classifier model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "rf = RandomForestClassifier(random_state = 42)\n",
        "\n",
        "# grid of parameters to search through when performing cross validation\n",
        "rf_params = {\n",
        "    'class_weight' : ['balanced']\n",
        "}\n",
        "\n",
        "# tests all permutations of the parameters outline in rf_params, returns the best performing model\n",
        "rf_model = GridSearchCV(estimator = rf,\n",
        "                        param_grid = rf_params,\n",
        "                        scoring = [\"accuracy\", \"f1_weighted\", \"roc_auc_ovr\"],\n",
        "                        refit = \"f1_weighted\",\n",
        "                        cv = 5,\n",
        "                        verbose = 3,\n",
        "                        return_train_score = True)\n",
        "\n",
        "# rf_model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "9a18V2MuQXB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "O4bSSnwGQfiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN Setup\n",
        "Import and install required libraries, sets some initial values."
      ],
      "metadata": {
        "id": "236PhyQolLI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import PyTorch and confirm version\n",
        "import torch\n",
        "from torch import nn\n",
        "print(\"Using PyTorch version: {}\".format(torch.__version__))\n",
        "\n",
        "# check the availability of and set the device\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(\"Using {} device.\".format(device))"
      ],
      "metadata": {
        "id": "cHhG7NZ9lahV",
        "outputId": "38791974-5921-4bfe-c610-9621edd0ae19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch version: 2.2.1+cu121\n",
            "\n",
            "Using cpu device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install Skorch, providing a wrapper for using PyTorch with Sklearn\n",
        "!pip install skorch\n",
        "\n",
        "# import Skorch and confirm version\n",
        "from skorch import __version__ as skorch_version\n",
        "from skorch import NeuralNetClassifier\n",
        "print(\"Using Skorch version: {}\".format(skorch_version))"
      ],
      "metadata": {
        "id": "GUSn5zCTQfGC",
        "outputId": "1dc72297-9640-4f2d-e595-e2a18f565110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: skorch in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.11.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (4.66.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (3.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Neural Network\n",
        "Currently using a Multilayer Perceptron (MLP), consider swapping to a hybrid model of a MLP and Convolutional Neural Network (CNN) later."
      ],
      "metadata": {
        "id": "AsNxj_cEDydF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_MLP(nn.Module):\n",
        "  \"\"\"Class that defines a multilayer perceptron model.\"\"\"\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    \"\"\"\n",
        "    Construct a new NN_MLP object.\n",
        "\n",
        "    Parameters:\n",
        "      input_size (int): number of inputs to the input layer.\n",
        "      hidden_size (int): number of inputs to the hidden layer(s).\n",
        "      output_size (int): number of outputs from the output layer, typically 1.\n",
        "    Returns:\n",
        "      : no value returned.\n",
        "    \"\"\"\n",
        "    super(NN_MLP, self).__init__()\n",
        "    # layers\n",
        "    self.h1 = nn.Linear(input_size, hidden_size)\n",
        "    self.h2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.output = nn.Linear(hidden_size, output_size)\n",
        "    # activation functions\n",
        "    self.relu = nn.ReLu()\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      X (Any): features to make prediction on.\n",
        "    Returns:\n",
        "      Any: predicted value.\n",
        "    \"\"\"\n",
        "    out = self.h1(X)\n",
        "    out = self.relu(out)\n",
        "    out = self.h2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.output(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "OFTGLP7jknbq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}