{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MGMnmhjzT-73",
        "gSVADpSUPvxs"
      ],
      "authorship_tag": "ABX9TyP7fMUrkooHy3B49pkbufIp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frosk-Kristian/COMP6002-Group10-Models/blob/develop/COMP6002_Group_Project_ML_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMP6002 Computer Science Project - Group 10\n",
        "Utilising Machine Learning to detect DDoS attacks.\n",
        "\n",
        "## Reference\n",
        "Iman Sharafaldin, Arash Habibi Lashkari, Saqib Hakak, and Ali A. Ghorbani, \"Developing Realistic Distributed Denial of Service (DDoS) Attack Dataset and Taxonomy\", IEEE 53rd International Carnahan Conference on Security Technology, Chennai, India, 2019."
      ],
      "metadata": {
        "id": "vkidwNbBPly0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "Run all of these first. Sets up libraries and directories used throughout notebook."
      ],
      "metadata": {
        "id": "zN1_GsQjkwEd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wgQiJbJiPMYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f9184d-8bfa-414b-926e-ffea19dd034b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Pandas version: 2.0.3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "print(f\"Using Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(f\"Using Numpy version: {np.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL7Gvu83G5FF",
        "outputId": "772b6c48-b2e7-4032-e740-8b1394c665a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Numpy version: 1.25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import __version__ as skl_ver\n",
        "print(f\"Using Sklearn version: {skl_ver}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg4eRaq0fNry",
        "outputId": "2cc77b3e-846b-46c2-b895-feac3de9605c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Sklearn version: 1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imblearn\n",
        "\n",
        "from imblearn import __version__ as imb_ver\n",
        "print(f\"Using Imbalanced learn version: {imb_ver}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO97MRm98Moc",
        "outputId": "9b12070d-91c2-4d0a-eb2d-83d890484c3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.4.0)\n",
            "Installing collected packages: imblearn\n",
            "Successfully installed imblearn-0.0\n",
            "Using Imbalanced learn version: 0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install codecarbon\n",
        "\n",
        "from codecarbon import __version__ as cc_ver\n",
        "print(f\"Using CodeCarbon.io version: {cc_ver}\")"
      ],
      "metadata": {
        "id": "kB8X6j51PPET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09396da5-7d8c-4326-eee2-60f69194254f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting codecarbon\n",
            "  Downloading codecarbon-2.3.5-py3-none-any.whl (174 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/174.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.6/174.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting arrow (from codecarbon)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.0.3)\n",
            "Collecting pynvml (from codecarbon)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.31.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from codecarbon) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from codecarbon) (9.0.0)\n",
            "Collecting rapidfuzz (from codecarbon)\n",
            "  Downloading rapidfuzz-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from codecarbon) (8.1.7)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon)\n",
            "  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (1.25.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
            "Installing collected packages: types-python-dateutil, rapidfuzz, pynvml, arrow, codecarbon\n",
            "Successfully installed arrow-1.3.0 codecarbon-2.3.5 pynvml-11.5.0 rapidfuzz-3.8.1 types-python-dateutil-2.9.0.20240316\n",
            "Using CodeCarbon.io version: 2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# mounts google drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# directory that project related files are stored in\n",
        "dl_dir = os.getcwd() + r'/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data'\n",
        "# directory that exports are saved in\n",
        "save_dir = dl_dir + f\"/Exports_{pd.Timestamp.today(tz='Australia/Perth').strftime('%d-%m-%Y')}\"\n",
        "# directory that the runtime will store the unzipped dataset in (not on your drive)\n",
        "data_dir = os.getcwd() + '/dataset'\n",
        "\n",
        "# checks if export directory already exists, if not creates it\n",
        "if os.path.exists(dl_dir):\n",
        "  print(f\"Directory {dl_dir} already exists.\\n\")\n",
        "else:\n",
        "  os.mkdir(dl_dir)\n",
        "  print(f\"Successfully created the directory {dl_dir}\")\n",
        "\n",
        "# checks if saves directory already exists, if not creates it\n",
        "if os.path.exists(save_dir):\n",
        "  print(f\"Directory {save_dir} already exists.\\n\")\n",
        "else:\n",
        "  os.mkdir(save_dir)\n",
        "  print(f\"Successfully created the directory {save_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUdWg_050_Ro",
        "outputId": "f0e79a55-72b8-42d6-969d-c8d4474cb5ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Directory /content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data already exists.\n",
            "\n",
            "Successfully created the directory /content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data/Exports_01-05-2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance Details\n",
        "Gets hardware specifications and region details of the Google Colab instance."
      ],
      "metadata": {
        "id": "MGMnmhjzT-73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU specifications\n",
        "from psutil import *\n",
        "\n",
        "!cat /proc/cpuinfo | grep \"model name\" | head -1\n",
        "\n",
        "print(f\"Number of CPU: {cpu_count()}\")\n",
        "!cat /proc/cpuinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW7ISKOxJFGT",
        "outputId": "44ec7f82-17d2-47f8-d1a2-8735007f55d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "Number of CPU: 2\n",
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VRAM specifications\n",
        "virtual_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LVrEUT0Ja_7",
        "outputId": "bf6f64aa-3f6a-4946-8d60-3bad6197439e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "svmem(total=13609451520, available=12364206080, percent=9.1, used=938582016, free=8320077824, active=670302208, inactive=4388851712, buffers=347938816, cached=4002852864, shared=1433600, slab=154607616)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU specifications\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM8VQOFEUKZ0",
        "outputId": "f528337e-d171-4877-9f65-eb389e08249c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Server information\n",
        "!curl ipinfo.io"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrBI5LScuDdB",
        "outputId": "e11f3e79-dee1-41a1-a800-2618dd7a28b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"ip\": \"35.230.111.187\",\n",
            "  \"hostname\": \"187.111.230.35.bc.googleusercontent.com\",\n",
            "  \"city\": \"The Dalles\",\n",
            "  \"region\": \"Oregon\",\n",
            "  \"country\": \"US\",\n",
            "  \"loc\": \"45.5946,-121.1787\",\n",
            "  \"org\": \"AS396982 Google LLC\",\n",
            "  \"postal\": \"97058\",\n",
            "  \"timezone\": \"America/Los_Angeles\",\n",
            "  \"readme\": \"https://ipinfo.io/missingauth\"\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Helper Functions"
      ],
      "metadata": {
        "id": "eEz2Qzc_kzkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions for exporting/importing models trained with Sklearn, use the NN specific functions when saving/loading the neural network\n",
        "# all functions defined will prompt the user for confirmation unless skip_prompt is True, to allow for skipping the functions when running the notebook\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "print(f\"Using Joblib version: {joblib.__version__}\")\n",
        "print(f\"Using Pickle version: {pickle.format_version}\")\n",
        "\n",
        "# function used to append a number to a filename in the event that the file already exists\n",
        "def IncrementFname(file_path):\n",
        "  \"\"\"\n",
        "  Takes a file path, splits the file name/extension and inserts an incrementing integer between them until a unique name is found.\n",
        "\n",
        "  Parameters:\n",
        "    file_path (string): full file path to increment, including extension.\n",
        "  Returns:\n",
        "    string: new file name in the form of 'path/to/file(i).extension'.\n",
        "  \"\"\"\n",
        "  fname, ext = os.path.splitext(file_path)\n",
        "  i = 1\n",
        "  new_name = f'{fname}({i}){ext}' # intial reassignment\n",
        "\n",
        "  # if filename(1).extension already exists, increment number until an unused one is found\n",
        "  while os.path.exists(new_name):\n",
        "    i += 1\n",
        "    new_name = f'{fname}({i}){ext}'\n",
        "\n",
        "  return new_name\n",
        "\n",
        "# Save a trained model to the provided filepath\n",
        "def SaveToFile(save_data, save_fpath, skip_prompt = False):\n",
        "  \"\"\"\n",
        "  Exports a object to a provided path using joblib, with or without prompting user for confirmation.\n",
        "\n",
        "  Parameters:\n",
        "    save_data (object): object to be saved.\n",
        "    save_fpath (string): file path that the object will be saved in, including filename and extension.\n",
        "    skip_prompt (boolean): if True, skips prompting user for confirmation before saving.\n",
        "  Returns:\n",
        "    : no value returned.\n",
        "  \"\"\"\n",
        "  to_save = \"y\" if skip_prompt else \"\"\n",
        "\n",
        "  # loops until 'y', 'Y', 'n', or 'N' have been entered by the user, returns early if 'n' or 'N' are entered.\n",
        "  while to_save.lower() not in ('y', 'n'):\n",
        "    to_save = input(\"Export to file? (y/n)\\n\")\n",
        "    if to_save.lower() in 'y':\n",
        "      print(\"Save confirmed.\")\n",
        "    elif to_save.lower() in 'n':\n",
        "      print(\"Save cancelled.\")\n",
        "      return\n",
        "    else:\n",
        "      print(\"WARNING: please only enter \\'y\\' to confirm or \\'n\\' to cancel.\")\n",
        "\n",
        "  fpath = save_fpath # allows for reassignment\n",
        "  if os.path.isfile(fpath):\n",
        "    print(f\"WARNING: file \\'{fpath}\\' already exists...\")\n",
        "    fpath = IncrementFname(fpath)\n",
        "    print(f\"...Saving to \\'{fpath}\\' instead.\")\n",
        "\n",
        "  try:\n",
        "    joblib.dump(save_data, fpath)\n",
        "  except:\n",
        "    print(f\"ERROR: an unknown error occured calling \\'joblib.dump({save_data}, {fpath})\\'!\")\n",
        "  else:\n",
        "    print(f\"SUCCESS: saved to \\'{fpath}\\'.\")\n",
        "\n",
        "\n",
        "# Load a trained model from the provided filepath\n",
        "def LoadFromFile(load_fpath, skip_prompt = False):\n",
        "  \"\"\"\n",
        "  Import a serialised object via joblib.\n",
        "\n",
        "  Parameters:\n",
        "    load_fpath (string): file path to stored object.\n",
        "    skip_prompt (boolean): if True, skips prompting user for confirmation before loading.\n",
        "  Returns:\n",
        "    object: if a file is found and loaded correctly, returns the stored object.\n",
        "    None: if a user enters 'n' to cancel, no matching file is found or an error occurs during loading, returns None.\n",
        "  \"\"\"\n",
        "  to_load = \"y\" if skip_prompt else \"\"\n",
        "\n",
        "  # loops until 'y', 'Y', 'n', or 'N' have been entered by the user, returns early if 'n' or 'N' are entered.\n",
        "  while to_load.lower() not in ('y', 'n'):\n",
        "    to_load = input(\"Do you wish to import from file? {y/n)\\n\")\n",
        "    if to_load.lower() in 'y':\n",
        "      print(\"Load confirmed.\")\n",
        "    elif to_load.lower() in 'n':\n",
        "      print(\"Load cancelled.\")\n",
        "      return None\n",
        "    else:\n",
        "      print(\"WARNING: please only enter \\'y\\' to confirm or \\'n\\' to cancel.\")\n",
        "\n",
        "  load_data = None\n",
        "  print(f\"Attempting to load object from \\'{load_fpath}\\'...\")\n",
        "  try:\n",
        "    load_data = joblib.load(load_fpath)\n",
        "  except FileNotFoundError:\n",
        "    print(f\"ERROR: the file \\'{load_fpath}\\' does not exist!\")\n",
        "    return None\n",
        "  except:\n",
        "    print(f\"ERROR: an unknown error occurred calling \\'joblib.load{load_fpath}\\'!\")\n",
        "    return None\n",
        "  else:\n",
        "    print(f\"SUCCESS: loaded data from \\'{load_fpath}\\'.\")\n",
        "\n",
        "  return load_data\n",
        "\n",
        "\n",
        "# save a neural network to file\n",
        "def SaveNN(nn_save, nn_fpath, skip_prompt = False):\n",
        "  \"\"\"\n",
        "  Exports a neural network to a provided path using pickle, with or without prompting user for confirmation.\n",
        "\n",
        "  Parameters:\n",
        "    nn_save (object): NeuralNetClassifier object to be saved.\n",
        "    nn_fpath (string): file path that the neural network will be saved in, including filename and extension.\n",
        "    skip_prompt (boolean): if True, skips prompting user for confirmation before saving.\n",
        "  Returns:\n",
        "    : no value returned.\n",
        "  \"\"\"\n",
        "  to_save = \"y\" if skip_prompt else \"\"\n",
        "\n",
        "  while to_save.lower() not in ('y', 'n'):\n",
        "    to_save = input(\"Save neural network to file? (y/n)\\n\")\n",
        "    if to_save.lower() in 'y':\n",
        "      print(\"Save confirmed.\")\n",
        "    elif to_save.lower() in 'n':\n",
        "      print(\"Save cancelled.\")\n",
        "      return\n",
        "    else:\n",
        "      print(\"WARNING: please only enter \\'y\\' to confirm or \\'n\\' to cancel.\")\n",
        "\n",
        "  fpath = nn_fpath # allows for reassignment\n",
        "  if os.path.isfile(fpath):\n",
        "    print(f\"WARNING: file \\'{fpath}\\' already exists...\")\n",
        "    fpath = IncrementFname(fpath)\n",
        "    print(f\"...Saving to \\'{fpath}\\' instead.\")\n",
        "\n",
        "  try:\n",
        "    with open(fpath, 'wb') as f:\n",
        "      pickle.dump(nn_save, f)\n",
        "  except:\n",
        "    print(\"ERROR: an unknown error occurred calling \\'pickle.dump(nn_save, f)\\'!\")\n",
        "  else:\n",
        "    print(f\"SUCCESS: saved neural network to \\'{fpath}\\'.\")\n",
        "\n",
        "\n",
        "# read a neural network previously saved to file\n",
        "def LoadNN(nn_fpath, skip_prompt = False):\n",
        "  \"\"\"\n",
        "  Import a serialised Skorch NeuralNetwork via Pickle.\n",
        "\n",
        "  Parameters:\n",
        "    nn_fpath (string): file path to stored neural network.\n",
        "    skip_prompt (boolean): if True, skips prompting user for confirmation before loading.\n",
        "  Returns:\n",
        "    object: if a file is found and loaded correctly, returns the stored Neural Network.\n",
        "    None: if a user enters 'n' to cancel, no matching file is found or an error occurs during loading, returns None.\n",
        "  \"\"\"\n",
        "  to_load = \"y\" if skip_prompt else \"\"\n",
        "\n",
        "  # loops until 'y', 'Y', 'n', or 'N' have been entered by the user, returns early if 'n' or 'N' are entered.\n",
        "  while to_load.lower() not in ('y', 'n'):\n",
        "    to_load = input(\"Do you wish to import a trained neural network? {y/n)\\n\")\n",
        "    if to_load.lower() in 'y':\n",
        "      print(\"Load confirmed.\")\n",
        "    elif to_load.lower() in 'n':\n",
        "      print(\"Load cancelled.\")\n",
        "      return None\n",
        "    else:\n",
        "      print(\"WARNING: please only enter \\'y\\' to confirm or \\'n\\' to cancel.\")\n",
        "\n",
        "  nn_load = None\n",
        "  print(f\"Attempting to load neural network from \\'{load_fpath}\\'...\")\n",
        "  try:\n",
        "    with open(nn_fpath, 'rb') as f:\n",
        "      nn_load = pickle.load(f)\n",
        "  except:\n",
        "    print(f\"ERROR: an unknown error occurred calling \\'picle.load{nn_fpath}\\'!\")\n",
        "    return None\n",
        "  else:\n",
        "    print(f\"SUCCESS: loaded neural network from \\'{load_fpath}\\'.\")\n",
        "\n",
        "  return nn_load"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otJrdxCQXWv5",
        "outputId": "ffd1f818-a9e1-4b04-edad-861b3389ba86"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Joblib version: 1.4.0\n",
            "Using Pickle version: 4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data\n",
        "Checks current working directory for datasets, if datasets are missing downloads a [.zip archive mirror of the CiCDDoS2019 hosted on Kaggle](https://www.kaggle.com/datasets/kristianfrossos/cicddos2019/data).\n",
        "\n",
        "**NOTE:** the first part of this section is specific to Google Colab, and will not work outside of it. Advise writing an alternative later for local use (relevant when training Neural Network for speed and when usage limits get in the way).\n"
      ],
      "metadata": {
        "id": "gSVADpSUPvxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "rVY0RkBfOV1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98aaaf24-3213-4fad-8c73-700a41540520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/79.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/79.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sets up kaggle environment variables (needed to access API)\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "\n",
        "# checks if kaggle key and username have been provided as secrets and sets environment variables appropriately\n",
        "# if not found, attempts to use kaggle.json\n",
        "try:\n",
        "  os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "  os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "  print(\"Using KAGGLE_KEY and KAGGLE_USERNAME defined in secrets.\")\n",
        "except (userdata.SecretNotFoundError, userdata.NotebookAccessError):\n",
        "  print(\"WARN: One or more secret(s) missing or inaccessible.\\n\")\n",
        "  if os.path.isfile('~/.kaggle/kaggle.json'):\n",
        "    print(\"Using existing kaggle.json\")\n",
        "  else:\n",
        "    print(\"Please upload kaggle.json\")\n",
        "    files.upload()\n",
        "\n",
        "    if os.path.isfile(os.getcwd() + '/content/kaggle.json'):\n",
        "      !rm -r ~/.kaggle\n",
        "      !mkdir ~/.kaggle\n",
        "      !mv ./kaggle.json ~/.kaggle/\n",
        "      !chmod 600 ~/.kaggle/kaggle.json\n",
        "    else:\n",
        "      print(\"\\'kaggle.json\\' not uploaded.\")\n",
        "      raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcOT1v4TO4rm",
        "outputId": "99ddb041-8398-4ea1-ffdc-5d545104f4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using KAGGLE_KEY and KAGGLE_USERNAME defined in secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checks if .zip archive containing dataset already exists in google drive and downloads it if necessary\n",
        "if os.path.isfile(dl_dir + '/cicddos2019.zip'):\n",
        "  print(\"Dataset already present.\")\n",
        "else:\n",
        "  print(f\"Downloading zipped dataset to {dl_dir}\")\n",
        "  !kaggle datasets download kristianfrossos/cicddos2019 -p {dl_dir.replace(' ', '\\ ')}\n",
        "\n",
        "# creates the content/dataset directory if it doesn't already exist\n",
        "if os.path.exists(data_dir):\n",
        "  print(f\"Directory {data_dir} already exists.\")\n",
        "else:\n",
        "  print(f\"Created directory: {data_dir}\")\n",
        "  os.mkdir(data_dir)\n",
        "\n",
        "# extracts contents of .zip archive to content/dataset if directory is not empty\n",
        "if not os.listdir(data_dir):\n",
        "  print(\"Empty directory, extracting dataset.\")\n",
        "  # unzips .zip archive\n",
        "  !unzip {dl_dir.replace(' ', '\\ ') + '/cicddos2019.zip'} -d {data_dir}\n",
        "else:\n",
        "  print(\"Non-empty directory, skipping download\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUQvxpnNMnRQ",
        "outputId": "4294dc0a-ea01-4b7c-b7d8-b53448189358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already present.\n",
            "Created directory: /content/dataset\n",
            "Empty directory, extracting dataset.\n",
            "Archive:  /content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data/cicddos2019.zip\n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_DNS.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_LDAP.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_MSSQL.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_NTP.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_NetBIOS.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_SNMP.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_SSDP.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/DrDoS_UDP.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/Syn.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/TFTP.csv  \n",
            "  inflating: /content/dataset/CSV-01-12/01-12/UDPLag.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/LDAP.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/MSSQL.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/NetBIOS.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/Portmap.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/Syn.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/UDP.csv  \n",
            "  inflating: /content/dataset/CSV-03-11/03-11/UDPLag.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialises empty list\n",
        "csv_list = []\n",
        "\n",
        "# iterates through all subdirectories of /content/dataset and appends the filepath of each .csv to csv_list\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "  for f in files:\n",
        "    if f.endswith(\".csv\"):\n",
        "      csv_list.append(os.path.join(root, f))\n",
        "\n",
        "# if .csv files were found, displays number of files and prints each path\n",
        "if not csv_list:\n",
        "  print(\"No .csv files found!\")\n",
        "else:\n",
        "  print(f\"{len(csv_list)} .csv files found.\")\n",
        "  for csv in csv_list:\n",
        "    print(csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMw9ryJSbiPL",
        "outputId": "5965f52f-3e63-4d29-fb64-da8f7e476d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18 .csv files found.\n",
            "/content/dataset/CSV-03-11/03-11/UDP.csv\n",
            "/content/dataset/CSV-03-11/03-11/NetBIOS.csv\n",
            "/content/dataset/CSV-03-11/03-11/UDPLag.csv\n",
            "/content/dataset/CSV-03-11/03-11/MSSQL.csv\n",
            "/content/dataset/CSV-03-11/03-11/Syn.csv\n",
            "/content/dataset/CSV-03-11/03-11/LDAP.csv\n",
            "/content/dataset/CSV-03-11/03-11/Portmap.csv\n",
            "/content/dataset/CSV-01-12/01-12/UDPLag.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_NTP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_UDP.csv\n",
            "/content/dataset/CSV-01-12/01-12/Syn.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_SSDP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_LDAP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_MSSQL.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_DNS.csv\n",
            "/content/dataset/CSV-01-12/01-12/TFTP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_SNMP.csv\n",
            "/content/dataset/CSV-01-12/01-12/DrDoS_NetBIOS.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Samples Dataset and Creates Subset"
      ],
      "metadata": {
        "id": "f0fI9HD0RaaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ddos_df = pd.DataFrame()\n",
        "benign_df = pd.DataFrame()\n",
        "\n",
        "for csv in csv_list:\n",
        "  data_iter = pd.read_csv(csv, chunksize=5000)\n",
        "  print(f\"Reading {csv}...\")\n",
        "\n",
        "  for chunk in data_iter:\n",
        "    ddos_rows = chunk[chunk[' Label'].str.lower() != 'benign']\n",
        "    benign_rows = chunk[chunk[' Label'].str.lower() == 'benign']\n",
        "\n",
        "    sample_size = min(len(ddos_rows), len(benign_rows)) // 25\n",
        "    ddos_sample = ddos_rows.sample(n=sample_size, random_state=42)\n",
        "    benign_sample = benign_rows.sample(n=sample_size, random_state=42)\n",
        "\n",
        "    ddos_df = pd.concat([ddos_df, ddos_sample], ignore_index=True)\n",
        "    benign_df = pd.concat([benign_df, benign_sample], ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14YJSfXt7F0Z",
        "outputId": "4763e89c-97c3-46ba-8a9b-1e88cea022f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/dataset/CSV-03-11/03-11/UDP.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/NetBIOS.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/UDPLag.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/MSSQL.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/Syn.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/LDAP.csv...\n",
            "Reading /content/dataset/CSV-03-11/03-11/Portmap.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/UDPLag.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_NTP.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_UDP.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/Syn.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_SSDP.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_LDAP.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_MSSQL.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_DNS.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/TFTP.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_SNMP.csv...\n",
            "Reading /content/dataset/CSV-01-12/01-12/DrDoS_NetBIOS.csv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenates ddos and benign dataframes into one subset\n",
        "subset = pd.concat([ddos_df, benign_df], ignore_index=True)\n",
        "\n",
        "# class weights\n",
        "weights = subset.value_counts(' Label', normalize=True)\n",
        "counts = subset.value_counts(' Label')\n",
        "\n",
        "# prints classes and their weights\n",
        "print(\"     Class      |     Weight     |     Count\")\n",
        "for index in weights.index:\n",
        "  print(f'{index:<15} | {(weights[index] * 100.0):<14n} | {counts[index]:n}')\n",
        "\n",
        "# prints total (for error checking)\n",
        "print(f\"{'Total':<15} | {sum(weights.values) * 100.0:<14n} | {sum(counts.values)}\")\n",
        "\n",
        "# stores unprocessed subset\n",
        "subset.to_csv((dl_dir + '/COMP6002_Raw_Subset.csv'), index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwKDB0qpF4Xz",
        "outputId": "23728bd4-b206-49a2-eb73-8858f66b342a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Class      |     Weight     |     Count\n",
            "BENIGN          | 50             | 1960\n",
            "Syn             | 12.9082        | 506\n",
            "TFTP            | 10.7653        | 422\n",
            "DrDoS_NTP       | 4.56633        | 179\n",
            "UDP-lag         | 3.59694        | 141\n",
            "UDP             | 2.37245        | 93\n",
            "DrDoS_DNS       | 2.37245        | 93\n",
            "Portmap         | 2.32143        | 91\n",
            "NetBIOS         | 2.16837        | 85\n",
            "MSSQL           | 1.83673        | 72\n",
            "LDAP            | 1.45408        | 57\n",
            "DrDoS_UDP       | 1.30102        | 51\n",
            "DrDoS_MSSQL     | 1.07143        | 42\n",
            "UDPLag          | 1.04592        | 41\n",
            "DrDoS_NetBIOS   | 0.739796       | 29\n",
            "DrDoS_SNMP      | 0.663265       | 26\n",
            "DrDoS_LDAP      | 0.433673       | 17\n",
            "DrDoS_SSDP      | 0.280612       | 11\n",
            "WebDDoS         | 0.102041       | 4\n",
            "Total           | 100            | 3920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "## Dropped Features\n",
        "*   Unnamed: 0: unknown feature.\n",
        "*   Flow Id: constructed from Source Ip, Destination Ip, Source Port, Destination Port and Protocol.\n",
        "*   Similar HTTP: object with no meaningful way to encode (the exact same objects won't necessarily exist in real data)"
      ],
      "metadata": {
        "id": "2IH9h-H8QULd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import ipaddress\n",
        "\n",
        "subset_path = dl_dir + '/COMP6002_Processed_Subset.csv'"
      ],
      "metadata": {
        "id": "W8uG3kVbderm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optionally import a unprocessed subset, skips above steps.\n",
        "raw_load = \"\"\n",
        "\n",
        "while raw_load.lower() not in ('y', 'n'):\n",
        "  raw_load = input(\"Do you wish to load a previously generated (but not processed) subset? (y/n)\\n\")\n",
        "\n",
        "  if raw_load.lower() in ('y'):\n",
        "    raw_path = dl_dir + '/COMP6002_Raw_Subset.csv'\n",
        "    try:\n",
        "      subset = pd.read_csv(raw_path)\n",
        "      print(f\"\\'{raw_path}\\' loaded successfully.\")\n",
        "    except:\n",
        "      print(f\"ERROR: an unknown error occured loading \\'{raw_path}\\'!\")\n",
        "  else:\n",
        "    if raw_load.lower() in ('n'):\n",
        "      print(\"Did not attempt to load existing data.\")\n",
        "    else:\n",
        "      print(\"WARNING: please only input \\'y\\' to load the data or \\'n\\' to skip loading.\")\n",
        "\n",
        "del raw_load"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAUF9N90JvnX",
        "outputId": "3840c8a7-7d23-4ab3-eea6-4d1983ae94b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you wish to load a previously generated (but not processed) subset? (y/n)\n",
            "n\n",
            "Did not attempt to load existing data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drops irrelevant columns\n",
        "subset.drop(columns = ['Unnamed: 0', 'Flow ID', 'SimillarHTTP'],\n",
        "            inplace = True)\n",
        "\n",
        "# replace infinite values with NaN so they are caught by the next 2 steps\n",
        "subset.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
        "\n",
        "# drop columns with at least 50% missing values\n",
        "subset.dropna(axis = 1,\n",
        "              thresh = int(0.5 * subset.shape[0]),\n",
        "              inplace = True)\n",
        "\n",
        "# replace missing values with the mean of their columns\n",
        "for col in subset.columns:\n",
        "  if subset[col].isna().sum() > 0:\n",
        "    subset[col].fillna(subset[col].mean(), inplace = True)\n",
        "\n",
        "# drop duplicate rows\n",
        "subset.drop_duplicates(inplace = True)\n",
        "\n",
        "# converts source and destination IP addresses to useable integer values\n",
        "subset['Source IP_int'] = subset.apply(lambda x: int (ipaddress.IPv4Address(x[' Source IP'])), axis=1)\n",
        "subset['Destination IP_int'] = subset.apply(lambda x: int (ipaddress.IPv4Address(x[' Destination IP'])), axis=1)\n",
        "\n",
        "# converts date and time values to unix timestamps\n",
        "subset['UnixTimestamp'] = subset.apply(lambda x: (pd.to_datetime(x[' Timestamp']).timestamp()), axis=1)\n",
        "\n",
        "# drops original columns\n",
        "subset.drop(columns = [' Source IP', ' Destination IP', ' Timestamp'],\n",
        "            inplace = True)"
      ],
      "metadata": {
        "id": "Ncl7SwVoSldZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Processed Data"
      ],
      "metadata": {
        "id": "qZskn0e5p4qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optionally save processed subset to .csv\n",
        "subset_save = \"\"\n",
        "\n",
        "while subset_save.lower() not in ('y', 'n'):\n",
        "  subset_save = input(\"Do you wish to export the subset to a .csv? (y/n)\")\n",
        "\n",
        "  if subset_save.lower() in ('y'):\n",
        "    new_subset_path = subset_path\n",
        "\n",
        "    if os.path.exists(new_subset_path):\n",
        "      new_subset_path = IncrementFname(subset_path)\n",
        "\n",
        "    try:\n",
        "      subset.to_csv(new_subset_path, index = False)\n",
        "      print(f\"Subset saved as \\'{new_subset_path}\\'.\")\n",
        "    except:\n",
        "      print(f\"ERROR: an unknown error occured saving \\'{new_subset_path}\\'!\")\n",
        "  else:\n",
        "    if subset_save.lower() in ('n'):\n",
        "      print(\"Skipped exporting to .csv file.\")\n",
        "    else:\n",
        "      print(\"WARNING: please only input \\'y\\' to save the data to a .csv or \\'n\\' to skip saving.\")\n",
        "\n",
        "del subset_save"
      ],
      "metadata": {
        "id": "WVs8OfgonhT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b24565f-9c71-4612-df95-be6da895a9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you wish to export the subset to a .csv? (y/n)y\n",
            "Subset saved as '/content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data/COMP6002_Processed_Subset.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Previously Processed Data"
      ],
      "metadata": {
        "id": "sTh9xpbpp8Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optionally import a preprocessed subset, skips above steps.\n",
        "subset_load = \"\"\n",
        "\n",
        "while subset_load.lower() not in ('y', 'n'):\n",
        "  subset_load = input(\"Do you wish to load a previously preprocessed subset? (y/n)\\n\")\n",
        "\n",
        "  if subset_load.lower() in ('y'):\n",
        "    subset_path = dl_dir + '/COMP6002_Processed_Subset.csv'\n",
        "    try:\n",
        "      subset = pd.read_csv(subset_path)\n",
        "      print(f\"\\'{subset_path}\\' loaded successfully.\")\n",
        "    except:\n",
        "      print(f\"ERROR: an unknown error occured loading \\'{subset_path}\\'!\")\n",
        "  else:\n",
        "    if subset_load.lower() in ('n'):\n",
        "      print(\"Did not attempt to load existing data.\")\n",
        "    else:\n",
        "      print(\"WARNING: please only input \\'y\\' to load the data or \\'n\\' to skip loading.\")\n",
        "\n",
        "del subset_load"
      ],
      "metadata": {
        "id": "SXVkn2lbDsTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430f2861-8ea0-4dca-c895-1de3e06fae58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you wish to load a previously preprocessed subset? (y/n)\n",
            "y\n",
            "'/content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data/COMP6002_Processed_Subset.csv' loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Training/Testing Data"
      ],
      "metadata": {
        "id": "XIZGXSiAqAHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splits subset across x and y axis\n",
        "X = subset.drop(columns = [' Label'], inplace = False)\n",
        "y = subset[' Label']"
      ],
      "metadata": {
        "id": "PGu_c9Dtd4FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set aside 20% of data to be used in testing, keeping the remaining 80% for training\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
      ],
      "metadata": {
        "id": "J2PH-oD5ljS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalise Data"
      ],
      "metadata": {
        "id": "zBp80dzJI3NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train[X_train.columns] = scaler.transform(X_train[X_train.columns])\n",
        "X_test[X_test.columns] = scaler.transform(X_test[X_test.columns])\n",
        "\n",
        "# exports scaler for future use\n",
        "SaveToFile(save_data = scaler, save_fpath = (save_dir + 'std_scaler.joblib'), skip_prompt = True)"
      ],
      "metadata": {
        "id": "7pnJ7yPbI5b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oversampling & Undersampling\n",
        "Using SMOTE for oversampling and edited nearest neighbours for undersampling."
      ],
      "metadata": {
        "id": "8niA7rBV8G_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "from imblearn.combine import SMOTEENN\n",
        "\n",
        "min_samples = y_train.value_counts().min()\n",
        "\n",
        "# oversampling\n",
        "smote = SMOTE(random_state = 42, k_neighbors = (min_samples - 1))\n",
        "\n",
        "# undersampling\n",
        "enn = EditedNearestNeighbours(sampling_strategy = 'majority', n_neighbors = min_samples)\n",
        "\n",
        "# combine the two\n",
        "smote_enn = SMOTEENN(sampling_strategy = 'not minority', random_state = 42, smote = smote, enn = enn)\n",
        "\n",
        "X_train_res, y_train_res = smote_enn.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "4voCdggw8JA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection"
      ],
      "metadata": {
        "id": "Qe8PpKdulNs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# \"base\" random forest model used for feature selection, so as to not require the actual rf we're using to be trained when attempting to train the neural network\n",
        "base_rf = RandomForestClassifier(class_weight = 'balanced', random_state = 42)\n",
        "selector = SelectFromModel(estimator = base_rf).fit(X_train_res, y_train_res)\n",
        "\n",
        "# keeps only the features identified by selector\n",
        "X_new = X_train_res[selector.get_feature_names_out()]\n",
        "\n",
        "# list selected features\n",
        "print(f\"Features remaining: {X_new.shape[1]}\")\n",
        "for col in X_new.columns:\n",
        "  print(f\"{col}\")\n",
        "\n",
        "del base_rf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxCAjhI8zsG4",
        "outputId": "385838a8-c8df-4df7-aa8f-2b17efa2c1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features remaining: 21\n",
            " Source Port\n",
            " Destination Port\n",
            "Total Length of Fwd Packets\n",
            " Fwd Packet Length Max\n",
            " Fwd Packet Length Min\n",
            " Fwd Packet Length Mean\n",
            "Flow Bytes/s\n",
            " Flow Packets/s\n",
            " Flow IAT Mean\n",
            " Flow IAT Min\n",
            " Fwd IAT Min\n",
            "Fwd Packets/s\n",
            " Min Packet Length\n",
            " Max Packet Length\n",
            " Packet Length Mean\n",
            " Average Packet Size\n",
            " Avg Fwd Segment Size\n",
            " Subflow Fwd Bytes\n",
            "Source IP_int\n",
            "Destination IP_int\n",
            "UnixTimestamp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_new.to_csv((save_dir + '/x.csv'), index = False)\n",
        "y_train_res.to_csv((save_dir + '/y.csv'), index = False)"
      ],
      "metadata": {
        "id": "YUzz6gKXtHgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Models"
      ],
      "metadata": {
        "id": "ZETBKB5rKWKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports needed for both models\n",
        "from sklearn.metrics import f1_score, accuracy_score as accuracy\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "46scG-TzMjMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "tracker = EmissionsTracker(project_name = \"detect_ddos_with_ml\")"
      ],
      "metadata": {
        "id": "t2SEczmRhLoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest\n",
        "**Values removed from param grid:**\n",
        "*   n_estimators:\n",
        "   *   100 - overfitting.\n",
        "*   max_depth:\n",
        "   *   None - overfitting.\n",
        "   *   20 - overfitting.\n",
        "   *   10 - overfitting.\n",
        "*   max_features:\n",
        "   *   'sqrt' - overfitting.\n",
        "   *   'log2' - overfitting.\n",
        "\n",
        "**Observations/Notes:**\n",
        "*  min_samples_split=10 could possibly be overfitting? Train/test accuracy are close together, but I don't trust anything that claims to be 100% accurate in training."
      ],
      "metadata": {
        "id": "60pYh9NWQbIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build random forest classifier model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(class_weight = 'balanced', n_jobs = -1, random_state = 42)\n",
        "\n",
        "# grid of parameters to search through when performing cross validation\n",
        "rf_params = {\n",
        "    'n_estimators' : [200, 500],\n",
        "    'max_depth' : [5],\n",
        "    'min_samples_leaf' : [1, 5, 10],\n",
        "    'min_samples_split' : [2, 5, 10],\n",
        "    'max_features' : [0.25, 0.5]\n",
        "}\n",
        "\n",
        "# tests all permutations of the parameters outline in rf_params, returns the best performing model\n",
        "rf_model = GridSearchCV(estimator = rf,\n",
        "                        param_grid = rf_params,\n",
        "                        scoring = [\"accuracy\", \"f1_weighted\"],\n",
        "                        refit = \"f1_weighted\",\n",
        "                        cv = 5,\n",
        "                        verbose = 3,\n",
        "                        return_train_score = True,\n",
        "                        n_jobs = 1)"
      ],
      "metadata": {
        "id": "9a18V2MuQXB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Random Forest"
      ],
      "metadata": {
        "id": "XLFFUMQVJvo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracker.start_task(\"train rf\")\n",
        "\n",
        "try:\n",
        "  # train model\n",
        "  rf_model.fit(X_new, y_train_res) # rf_model.fit(X_train_res, y_train_res)\n",
        "finally:\n",
        "  rf_emissions = tracker.stop_task(\"train rf\")"
      ],
      "metadata": {
        "id": "Rfd2t83SVBfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17266d7b-2c05-40b8-8041-c8714f675bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=2, n_estimators=200; accuracy: (train=0.859, test=0.857) f1_weighted: (train=0.857, test=0.857) total time=   8.9s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=2, n_estimators=200; accuracy: (train=0.853, test=0.849) f1_weighted: (train=0.851, test=0.847) total time=   9.5s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=2, n_estimators=200; accuracy: (train=0.850, test=0.852) f1_weighted: (train=0.848, test=0.849) total time=  10.5s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=2, n_estimators=200; accuracy: (train=0.849, test=0.843) f1_weighted: (train=0.848, test=0.840) total time=  10.6s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=2, n_estimators=200; accuracy: (train=0.867, test=0.866) f1_weighted: (train=0.865, test=0.864) total time=   8.4s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=2, n_estimators=500; accuracy: (train=0.856, test=0.854) f1_weighted: (train=0.855, test=0.854) total time=  25.2s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=2, n_estimators=500; accuracy: (train=0.856, test=0.853) f1_weighted: (train=0.854, test=0.851) total time=  25.2s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=2, n_estimators=500; accuracy: (train=0.857, test=0.859) f1_weighted: (train=0.856, test=0.857) total time=  25.3s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=2, n_estimators=500; accuracy: (train=0.849, test=0.844) f1_weighted: (train=0.847, test=0.841) total time=  23.5s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=2, n_estimators=500; accuracy: (train=0.854, test=0.856) f1_weighted: (train=0.852, test=0.855) total time=  23.6s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=5, n_estimators=200; accuracy: (train=0.856, test=0.856) f1_weighted: (train=0.855, test=0.855) total time=  10.7s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=5, n_estimators=200; accuracy: (train=0.852, test=0.848) f1_weighted: (train=0.849, test=0.845) total time=  10.7s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=5, n_estimators=200; accuracy: (train=0.850, test=0.850) f1_weighted: (train=0.848, test=0.847) total time=   9.0s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=5, n_estimators=200; accuracy: (train=0.847, test=0.841) f1_weighted: (train=0.845, test=0.838) total time=   9.4s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=5, n_estimators=200; accuracy: (train=0.865, test=0.864) f1_weighted: (train=0.863, test=0.862) total time=  10.7s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=5, n_estimators=500; accuracy: (train=0.857, test=0.856) f1_weighted: (train=0.856, test=0.855) total time=  25.5s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=5, n_estimators=500; accuracy: (train=0.857, test=0.852) f1_weighted: (train=0.855, test=0.850) total time=  23.4s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=5, n_estimators=500; accuracy: (train=0.857, test=0.858) f1_weighted: (train=0.855, test=0.856) total time=  22.9s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=5, n_estimators=500; accuracy: (train=0.849, test=0.844) f1_weighted: (train=0.847, test=0.841) total time=  25.2s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=5, n_estimators=500; accuracy: (train=0.853, test=0.854) f1_weighted: (train=0.852, test=0.853) total time=  25.3s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=10, n_estimators=200; accuracy: (train=0.854, test=0.854) f1_weighted: (train=0.852, test=0.853) total time=   9.9s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=10, n_estimators=200; accuracy: (train=0.856, test=0.849) f1_weighted: (train=0.854, test=0.847) total time=   8.4s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=10, n_estimators=200; accuracy: (train=0.850, test=0.850) f1_weighted: (train=0.848, test=0.848) total time=  10.5s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=10, n_estimators=200; accuracy: (train=0.842, test=0.835) f1_weighted: (train=0.840, test=0.832) total time=  10.5s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=10, n_estimators=200; accuracy: (train=0.864, test=0.862) f1_weighted: (train=0.861, test=0.859) total time=   8.9s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=10, n_estimators=500; accuracy: (train=0.857, test=0.856) f1_weighted: (train=0.856, test=0.855) total time=  24.2s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=10, n_estimators=500; accuracy: (train=0.857, test=0.852) f1_weighted: (train=0.856, test=0.850) total time=  24.8s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=10, n_estimators=500; accuracy: (train=0.856, test=0.855) f1_weighted: (train=0.854, test=0.853) total time=  24.9s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=10, n_estimators=500; accuracy: (train=0.849, test=0.844) f1_weighted: (train=0.846, test=0.840) total time=  23.5s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=1, min_samples_split=10, n_estimators=500; accuracy: (train=0.856, test=0.856) f1_weighted: (train=0.854, test=0.854) total time=  23.5s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=2, n_estimators=200; accuracy: (train=0.854, test=0.854) f1_weighted: (train=0.852, test=0.853) total time=  10.7s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=2, n_estimators=200; accuracy: (train=0.854, test=0.850) f1_weighted: (train=0.852, test=0.848) total time=  10.3s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=2, n_estimators=200; accuracy: (train=0.852, test=0.853) f1_weighted: (train=0.850, test=0.850) total time=   8.9s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=2, n_estimators=200; accuracy: (train=0.843, test=0.837) f1_weighted: (train=0.840, test=0.833) total time=   9.2s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=2, n_estimators=200; accuracy: (train=0.857, test=0.857) f1_weighted: (train=0.855, test=0.855) total time=  10.4s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=2, n_estimators=500; accuracy: (train=0.857, test=0.857) f1_weighted: (train=0.856, test=0.856) total time=  25.6s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=2, n_estimators=500; accuracy: (train=0.858, test=0.853) f1_weighted: (train=0.856, test=0.852) total time=  22.9s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=2, n_estimators=500; accuracy: (train=0.852, test=0.854) f1_weighted: (train=0.850, test=0.851) total time=  23.8s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=2, n_estimators=500; accuracy: (train=0.848, test=0.844) f1_weighted: (train=0.846, test=0.840) total time=  25.2s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=2, n_estimators=500; accuracy: (train=0.856, test=0.856) f1_weighted: (train=0.854, test=0.854) total time=  25.1s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=5, n_estimators=200; accuracy: (train=0.854, test=0.854) f1_weighted: (train=0.852, test=0.853) total time=   9.2s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=5, n_estimators=200; accuracy: (train=0.854, test=0.850) f1_weighted: (train=0.852, test=0.848) total time=   9.1s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=5, n_estimators=200; accuracy: (train=0.852, test=0.853) f1_weighted: (train=0.850, test=0.850) total time=  10.3s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=5, n_estimators=200; accuracy: (train=0.843, test=0.837) f1_weighted: (train=0.840, test=0.833) total time=  10.6s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=5, n_estimators=200; accuracy: (train=0.857, test=0.857) f1_weighted: (train=0.855, test=0.855) total time=   8.5s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=5, n_estimators=500; accuracy: (train=0.857, test=0.857) f1_weighted: (train=0.856, test=0.856) total time=  25.3s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=5, n_estimators=500; accuracy: (train=0.858, test=0.853) f1_weighted: (train=0.856, test=0.852) total time=  24.7s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=5, n_estimators=500; accuracy: (train=0.852, test=0.854) f1_weighted: (train=0.850, test=0.851) total time=  24.0s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=5, n_estimators=500; accuracy: (train=0.848, test=0.844) f1_weighted: (train=0.846, test=0.840) total time=  23.1s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=5, n_estimators=500; accuracy: (train=0.856, test=0.856) f1_weighted: (train=0.854, test=0.854) total time=  24.4s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=10, n_estimators=200; accuracy: (train=0.854, test=0.854) f1_weighted: (train=0.852, test=0.853) total time=  10.7s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=10, n_estimators=200; accuracy: (train=0.854, test=0.850) f1_weighted: (train=0.852, test=0.848) total time=  10.3s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=10, n_estimators=200; accuracy: (train=0.852, test=0.853) f1_weighted: (train=0.850, test=0.850) total time=   8.4s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=10, n_estimators=200; accuracy: (train=0.843, test=0.837) f1_weighted: (train=0.840, test=0.833) total time=  10.5s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=10, n_estimators=200; accuracy: (train=0.857, test=0.857) f1_weighted: (train=0.855, test=0.855) total time=  10.6s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=10, n_estimators=500; accuracy: (train=0.857, test=0.857) f1_weighted: (train=0.856, test=0.856) total time=  23.6s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=10, n_estimators=500; accuracy: (train=0.858, test=0.853) f1_weighted: (train=0.856, test=0.852) total time=  23.0s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=10, n_estimators=500; accuracy: (train=0.852, test=0.854) f1_weighted: (train=0.850, test=0.851) total time=  24.9s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=10, n_estimators=500; accuracy: (train=0.848, test=0.844) f1_weighted: (train=0.846, test=0.840) total time=  25.1s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=5, min_samples_split=10, n_estimators=500; accuracy: (train=0.856, test=0.856) f1_weighted: (train=0.854, test=0.854) total time=  23.9s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=2, n_estimators=200; accuracy: (train=0.857, test=0.856) f1_weighted: (train=0.855, test=0.855) total time=   8.5s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=2, n_estimators=200; accuracy: (train=0.855, test=0.850) f1_weighted: (train=0.853, test=0.848) total time=  10.5s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=2, n_estimators=200; accuracy: (train=0.858, test=0.860) f1_weighted: (train=0.857, test=0.858) total time=  10.3s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=2, n_estimators=200; accuracy: (train=0.843, test=0.837) f1_weighted: (train=0.840, test=0.833) total time=   8.8s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=2, n_estimators=200; accuracy: (train=0.848, test=0.851) f1_weighted: (train=0.846, test=0.849) total time=   9.2s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=2, n_estimators=500; accuracy: (train=0.859, test=0.857) f1_weighted: (train=0.857, test=0.856) total time=  25.3s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=2, n_estimators=500; accuracy: (train=0.856, test=0.850) f1_weighted: (train=0.854, test=0.848) total time=  24.7s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=2, n_estimators=500; accuracy: (train=0.857, test=0.859) f1_weighted: (train=0.856, test=0.857) total time=  22.9s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=2, n_estimators=500; accuracy: (train=0.849, test=0.843) f1_weighted: (train=0.847, test=0.840) total time=  23.6s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=2, n_estimators=500; accuracy: (train=0.855, test=0.856) f1_weighted: (train=0.854, test=0.854) total time=  24.9s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=5, n_estimators=200; accuracy: (train=0.857, test=0.856) f1_weighted: (train=0.855, test=0.855) total time=  10.7s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=5, n_estimators=200; accuracy: (train=0.855, test=0.850) f1_weighted: (train=0.853, test=0.848) total time=   8.5s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=5, n_estimators=200; accuracy: (train=0.858, test=0.860) f1_weighted: (train=0.857, test=0.858) total time=  10.1s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=5, n_estimators=200; accuracy: (train=0.843, test=0.837) f1_weighted: (train=0.840, test=0.833) total time=  10.6s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=5, n_estimators=200; accuracy: (train=0.848, test=0.851) f1_weighted: (train=0.846, test=0.849) total time=   9.7s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=5, n_estimators=500; accuracy: (train=0.859, test=0.857) f1_weighted: (train=0.857, test=0.856) total time=  23.2s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=5, n_estimators=500; accuracy: (train=0.856, test=0.850) f1_weighted: (train=0.854, test=0.848) total time=  24.9s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=5, n_estimators=500; accuracy: (train=0.857, test=0.859) f1_weighted: (train=0.856, test=0.857) total time=  24.8s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=5, n_estimators=500; accuracy: (train=0.849, test=0.843) f1_weighted: (train=0.847, test=0.840) total time=  23.9s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=5, n_estimators=500; accuracy: (train=0.855, test=0.856) f1_weighted: (train=0.854, test=0.854) total time=  22.8s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=10, n_estimators=200; accuracy: (train=0.857, test=0.856) f1_weighted: (train=0.855, test=0.855) total time=  10.5s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=10, n_estimators=200; accuracy: (train=0.855, test=0.850) f1_weighted: (train=0.853, test=0.848) total time=  10.6s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=10, n_estimators=200; accuracy: (train=0.858, test=0.860) f1_weighted: (train=0.857, test=0.858) total time=   9.0s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=10, n_estimators=200; accuracy: (train=0.843, test=0.837) f1_weighted: (train=0.840, test=0.833) total time=   9.2s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=10, n_estimators=200; accuracy: (train=0.848, test=0.851) f1_weighted: (train=0.846, test=0.849) total time=  10.4s\n",
            "[CV 1/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=10, n_estimators=500; accuracy: (train=0.859, test=0.857) f1_weighted: (train=0.857, test=0.856) total time=  25.1s\n",
            "[CV 2/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=10, n_estimators=500; accuracy: (train=0.856, test=0.850) f1_weighted: (train=0.854, test=0.848) total time=  22.6s\n",
            "[CV 3/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=10, n_estimators=500; accuracy: (train=0.857, test=0.859) f1_weighted: (train=0.856, test=0.857) total time=  24.1s\n",
            "[CV 4/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=10, n_estimators=500; accuracy: (train=0.849, test=0.843) f1_weighted: (train=0.847, test=0.840) total time=  24.9s\n",
            "[CV 5/5] END max_depth=5, max_features=0.25, min_samples_leaf=10, min_samples_split=10, n_estimators=500; accuracy: (train=0.855, test=0.856) f1_weighted: (train=0.854, test=0.854) total time=  24.7s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; accuracy: (train=0.936, test=0.932) f1_weighted: (train=0.935, test=0.931) total time=  17.9s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; accuracy: (train=0.928, test=0.925) f1_weighted: (train=0.927, test=0.924) total time=  17.6s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; accuracy: (train=0.937, test=0.938) f1_weighted: (train=0.936, test=0.937) total time=  19.1s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; accuracy: (train=0.939, test=0.942) f1_weighted: (train=0.938, test=0.941) total time=  17.7s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; accuracy: (train=0.916, test=0.915) f1_weighted: (train=0.915, test=0.913) total time=  17.8s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=500; accuracy: (train=0.928, test=0.924) f1_weighted: (train=0.926, test=0.923) total time=  45.5s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=500; accuracy: (train=0.937, test=0.931) f1_weighted: (train=0.936, test=0.930) total time=  44.9s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=500; accuracy: (train=0.932, test=0.933) f1_weighted: (train=0.931, test=0.932) total time=  44.8s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=500; accuracy: (train=0.945, test=0.949) f1_weighted: (train=0.944, test=0.948) total time=  44.8s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=500; accuracy: (train=0.920, test=0.921) f1_weighted: (train=0.919, test=0.920) total time=  45.2s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; accuracy: (train=0.927, test=0.925) f1_weighted: (train=0.926, test=0.924) total time=  17.8s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; accuracy: (train=0.932, test=0.929) f1_weighted: (train=0.931, test=0.928) total time=  17.6s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; accuracy: (train=0.937, test=0.938) f1_weighted: (train=0.936, test=0.937) total time=  19.6s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; accuracy: (train=0.934, test=0.938) f1_weighted: (train=0.934, test=0.937) total time=  17.7s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; accuracy: (train=0.915, test=0.913) f1_weighted: (train=0.913, test=0.912) total time=  17.6s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=500; accuracy: (train=0.936, test=0.932) f1_weighted: (train=0.935, test=0.932) total time=  45.4s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=500; accuracy: (train=0.937, test=0.931) f1_weighted: (train=0.936, test=0.930) total time=  45.2s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=500; accuracy: (train=0.932, test=0.933) f1_weighted: (train=0.931, test=0.932) total time=  44.7s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=500; accuracy: (train=0.946, test=0.949) f1_weighted: (train=0.945, test=0.949) total time=  45.4s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=500; accuracy: (train=0.919, test=0.919) f1_weighted: (train=0.918, test=0.918) total time=  45.4s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; accuracy: (train=0.934, test=0.929) f1_weighted: (train=0.932, test=0.928) total time=  17.9s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; accuracy: (train=0.934, test=0.928) f1_weighted: (train=0.934, test=0.927) total time=  18.6s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; accuracy: (train=0.938, test=0.939) f1_weighted: (train=0.937, test=0.938) total time=  18.2s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; accuracy: (train=0.945, test=0.949) f1_weighted: (train=0.944, test=0.949) total time=  17.6s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; accuracy: (train=0.915, test=0.914) f1_weighted: (train=0.913, test=0.913) total time=  17.6s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=500; accuracy: (train=0.932, test=0.928) f1_weighted: (train=0.931, test=0.926) total time=  45.2s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=500; accuracy: (train=0.932, test=0.926) f1_weighted: (train=0.931, test=0.924) total time=  45.4s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=500; accuracy: (train=0.932, test=0.933) f1_weighted: (train=0.931, test=0.932) total time=  44.8s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=500; accuracy: (train=0.946, test=0.949) f1_weighted: (train=0.945, test=0.949) total time=  44.7s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=500; accuracy: (train=0.919, test=0.919) f1_weighted: (train=0.917, test=0.917) total time=  45.0s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=2, n_estimators=200; accuracy: (train=0.935, test=0.930) f1_weighted: (train=0.934, test=0.929) total time=  18.0s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=2, n_estimators=200; accuracy: (train=0.936, test=0.929) f1_weighted: (train=0.936, test=0.928) total time=  19.3s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=2, n_estimators=200; accuracy: (train=0.940, test=0.941) f1_weighted: (train=0.940, test=0.940) total time=  17.5s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=2, n_estimators=200; accuracy: (train=0.931, test=0.933) f1_weighted: (train=0.929, test=0.932) total time=  17.7s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=2, n_estimators=200; accuracy: (train=0.915, test=0.914) f1_weighted: (train=0.913, test=0.913) total time=  18.1s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=2, n_estimators=500; accuracy: (train=0.929, test=0.923) f1_weighted: (train=0.928, test=0.922) total time=  46.9s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=2, n_estimators=500; accuracy: (train=0.933, test=0.926) f1_weighted: (train=0.932, test=0.925) total time=  45.5s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=2, n_estimators=500; accuracy: (train=0.934, test=0.935) f1_weighted: (train=0.933, test=0.933) total time=  45.4s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=2, n_estimators=500; accuracy: (train=0.947, test=0.951) f1_weighted: (train=0.947, test=0.950) total time=  45.1s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=2, n_estimators=500; accuracy: (train=0.920, test=0.920) f1_weighted: (train=0.918, test=0.918) total time=  45.5s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=200; accuracy: (train=0.935, test=0.930) f1_weighted: (train=0.934, test=0.929) total time=  19.7s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=200; accuracy: (train=0.936, test=0.929) f1_weighted: (train=0.936, test=0.928) total time=  17.9s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=200; accuracy: (train=0.940, test=0.941) f1_weighted: (train=0.940, test=0.940) total time=  17.8s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=200; accuracy: (train=0.931, test=0.933) f1_weighted: (train=0.929, test=0.932) total time=  18.0s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=200; accuracy: (train=0.915, test=0.914) f1_weighted: (train=0.913, test=0.913) total time=  19.2s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=500; accuracy: (train=0.929, test=0.923) f1_weighted: (train=0.928, test=0.922) total time=  45.8s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=500; accuracy: (train=0.933, test=0.926) f1_weighted: (train=0.932, test=0.925) total time=  45.6s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=500; accuracy: (train=0.934, test=0.935) f1_weighted: (train=0.933, test=0.933) total time=  45.3s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=500; accuracy: (train=0.947, test=0.951) f1_weighted: (train=0.947, test=0.950) total time=  45.5s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=5, n_estimators=500; accuracy: (train=0.920, test=0.920) f1_weighted: (train=0.918, test=0.918) total time=  46.0s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=10, n_estimators=200; accuracy: (train=0.935, test=0.930) f1_weighted: (train=0.934, test=0.929) total time=  18.1s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=10, n_estimators=200; accuracy: (train=0.936, test=0.929) f1_weighted: (train=0.936, test=0.928) total time=  17.9s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=10, n_estimators=200; accuracy: (train=0.940, test=0.941) f1_weighted: (train=0.940, test=0.940) total time=  18.8s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=10, n_estimators=200; accuracy: (train=0.931, test=0.933) f1_weighted: (train=0.929, test=0.932) total time=  18.4s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=10, n_estimators=200; accuracy: (train=0.915, test=0.914) f1_weighted: (train=0.913, test=0.913) total time=  17.9s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=10, n_estimators=500; accuracy: (train=0.929, test=0.923) f1_weighted: (train=0.928, test=0.922) total time=  46.3s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=10, n_estimators=500; accuracy: (train=0.933, test=0.926) f1_weighted: (train=0.932, test=0.925) total time=  46.2s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=10, n_estimators=500; accuracy: (train=0.934, test=0.935) f1_weighted: (train=0.933, test=0.933) total time=  45.9s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=10, n_estimators=500; accuracy: (train=0.947, test=0.951) f1_weighted: (train=0.947, test=0.950) total time=  45.9s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=5, min_samples_split=10, n_estimators=500; accuracy: (train=0.920, test=0.920) f1_weighted: (train=0.918, test=0.918) total time=  45.6s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=2, n_estimators=200; accuracy: (train=0.939, test=0.934) f1_weighted: (train=0.938, test=0.933) total time=  18.4s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=2, n_estimators=200; accuracy: (train=0.946, test=0.940) f1_weighted: (train=0.945, test=0.939) total time=  18.9s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=2, n_estimators=200; accuracy: (train=0.940, test=0.941) f1_weighted: (train=0.939, test=0.940) total time=  17.8s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=2, n_estimators=200; accuracy: (train=0.934, test=0.936) f1_weighted: (train=0.933, test=0.934) total time=  17.8s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=2, n_estimators=200; accuracy: (train=0.913, test=0.914) f1_weighted: (train=0.912, test=0.912) total time=  19.7s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=2, n_estimators=500; accuracy: (train=0.933, test=0.928) f1_weighted: (train=0.932, test=0.927) total time=  45.9s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=2, n_estimators=500; accuracy: (train=0.934, test=0.928) f1_weighted: (train=0.933, test=0.927) total time=  45.3s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=2, n_estimators=500; accuracy: (train=0.935, test=0.938) f1_weighted: (train=0.934, test=0.936) total time=  45.3s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=2, n_estimators=500; accuracy: (train=0.946, test=0.950) f1_weighted: (train=0.945, test=0.950) total time=  45.5s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=2, n_estimators=500; accuracy: (train=0.920, test=0.920) f1_weighted: (train=0.919, test=0.920) total time=  45.4s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=5, n_estimators=200; accuracy: (train=0.939, test=0.934) f1_weighted: (train=0.938, test=0.933) total time=  19.8s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=5, n_estimators=200; accuracy: (train=0.946, test=0.940) f1_weighted: (train=0.945, test=0.939) total time=  17.9s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=5, n_estimators=200; accuracy: (train=0.940, test=0.941) f1_weighted: (train=0.939, test=0.940) total time=  17.8s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=5, n_estimators=200; accuracy: (train=0.934, test=0.936) f1_weighted: (train=0.933, test=0.934) total time=  19.0s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=5, n_estimators=200; accuracy: (train=0.913, test=0.914) f1_weighted: (train=0.912, test=0.912) total time=  18.0s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=5, n_estimators=500; accuracy: (train=0.933, test=0.928) f1_weighted: (train=0.932, test=0.927) total time=  45.9s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=5, n_estimators=500; accuracy: (train=0.934, test=0.928) f1_weighted: (train=0.933, test=0.927) total time=  45.5s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=5, n_estimators=500; accuracy: (train=0.935, test=0.938) f1_weighted: (train=0.934, test=0.936) total time=  45.3s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=5, n_estimators=500; accuracy: (train=0.946, test=0.950) f1_weighted: (train=0.945, test=0.950) total time=  46.3s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=5, n_estimators=500; accuracy: (train=0.920, test=0.920) f1_weighted: (train=0.919, test=0.920) total time=  45.7s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=10, n_estimators=200; accuracy: (train=0.939, test=0.934) f1_weighted: (train=0.938, test=0.933) total time=  18.1s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=10, n_estimators=200; accuracy: (train=0.946, test=0.940) f1_weighted: (train=0.945, test=0.939) total time=  18.0s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=10, n_estimators=200; accuracy: (train=0.940, test=0.941) f1_weighted: (train=0.939, test=0.940) total time=  19.6s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=10, n_estimators=200; accuracy: (train=0.934, test=0.936) f1_weighted: (train=0.933, test=0.934) total time=  18.0s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=10, n_estimators=200; accuracy: (train=0.913, test=0.914) f1_weighted: (train=0.912, test=0.912) total time=  18.1s\n",
            "[CV 1/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=10, n_estimators=500; accuracy: (train=0.933, test=0.928) f1_weighted: (train=0.932, test=0.927) total time=  46.0s\n",
            "[CV 2/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=10, n_estimators=500; accuracy: (train=0.934, test=0.928) f1_weighted: (train=0.933, test=0.927) total time=  45.7s\n",
            "[CV 3/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=10, n_estimators=500; accuracy: (train=0.935, test=0.938) f1_weighted: (train=0.934, test=0.936) total time=  45.2s\n",
            "[CV 4/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=10, n_estimators=500; accuracy: (train=0.946, test=0.950) f1_weighted: (train=0.945, test=0.950) total time=  45.2s\n",
            "[CV 5/5] END max_depth=5, max_features=0.5, min_samples_leaf=10, min_samples_split=10, n_estimators=500; accuracy: (train=0.920, test=0.920) f1_weighted: (train=0.919, test=0.920) total time=  45.3s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=RandomForestClassifier(class_weight='balanced',\n",
              "                                              n_jobs=-1, random_state=42),\n",
              "             n_jobs=1,\n",
              "             param_grid={'max_depth': [5], 'max_features': [0.25, 0.5],\n",
              "                         'min_samples_leaf': [1, 5, 10],\n",
              "                         'min_samples_split': [2, 5, 10],\n",
              "                         'n_estimators': [200, 500]},\n",
              "             refit='f1_weighted', return_train_score=True,\n",
              "             scoring=['accuracy', 'f1_weighted'], verbose=3)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=RandomForestClassifier(class_weight=&#x27;balanced&#x27;,\n",
              "                                              n_jobs=-1, random_state=42),\n",
              "             n_jobs=1,\n",
              "             param_grid={&#x27;max_depth&#x27;: [5], &#x27;max_features&#x27;: [0.25, 0.5],\n",
              "                         &#x27;min_samples_leaf&#x27;: [1, 5, 10],\n",
              "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
              "                         &#x27;n_estimators&#x27;: [200, 500]},\n",
              "             refit=&#x27;f1_weighted&#x27;, return_train_score=True,\n",
              "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_weighted&#x27;], verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=RandomForestClassifier(class_weight=&#x27;balanced&#x27;,\n",
              "                                              n_jobs=-1, random_state=42),\n",
              "             n_jobs=1,\n",
              "             param_grid={&#x27;max_depth&#x27;: [5], &#x27;max_features&#x27;: [0.25, 0.5],\n",
              "                         &#x27;min_samples_leaf&#x27;: [1, 5, 10],\n",
              "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
              "                         &#x27;n_estimators&#x27;: [200, 500]},\n",
              "             refit=&#x27;f1_weighted&#x27;, return_train_score=True,\n",
              "             scoring=[&#x27;accuracy&#x27;, &#x27;f1_weighted&#x27;], verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, n_jobs=-1, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, n_jobs=-1, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training results\n",
        "rf_train = rf_model.predict(X_train_res)\n",
        "print(f'Emissions: {rf_emissions} kg CO₂')\n",
        "\n",
        "# displays the best model produced in training as well as its' hyperparameters and f1 score\n",
        "print(f'Training Result:\\n Best Model: {rf_model.best_estimator_}\\n Best Parameters: {rf_model.best_params_}\\n F1 Score: {rf_model.best_score_}')\n",
        "\n",
        "# evaluate the models performance and display scores\n",
        "print(f'Random Forest (TRAINING):\\n accuracy: {accuracy(y_train_res, rf_train):f}\\n f1 score: {f1_score(y_train_res, rf_train, average=\"weighted\"):f}')"
      ],
      "metadata": {
        "id": "VXbnWb1zK2tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d53904f-fd7c-4c23-e9db-819a9688c7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Result:\n",
            " Best Model: RandomForestClassifier(class_weight='balanced', max_depth=5, max_features=0.5,\n",
            "                       min_samples_split=5, n_estimators=500, n_jobs=-1,\n",
            "                       random_state=42)\n",
            " Best Parameters: {'max_depth': 5, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 500}\n",
            " F1 Score: 0.9320425071898889\n",
            "Random Forest (TRAINING):\n",
            " accuracy: 0.924771\n",
            " f1 score: 0.923597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Random Forest"
      ],
      "metadata": {
        "id": "_xyy5kb4JxiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on test data\n",
        "rf_test = rf_model.predict(X_test)\n",
        "\n",
        "# evaluate the models performance and display scores\n",
        "print(f'Random Forest (TESTING):\\n accuracy: {accuracy(y_test, rf_test):f}\\n f1 score: {f1_score(y_test, rf_test, average=\"weighted\"):f}')"
      ],
      "metadata": {
        "id": "vyU9kN8kJ1FU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ebaef0-2be0-4271-b5a4-4913f7949c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest (TESTING):\n",
            " accuracy: 0.828863\n",
            " f1 score: 0.846268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export Model"
      ],
      "metadata": {
        "id": "lH21zCuVPyZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uses joblib to serialise trained random forest model\n",
        "SaveToFile(save_data = rf_model, save_fpath = (save_dir + \"/random_forest.joblib\"), skip_prompt = True)"
      ],
      "metadata": {
        "id": "DdnGauFPP7TK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d41cc840-e946-436b-bc24-7372e7867e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you wish to save the trained model? (y/n)\n",
            "y\n",
            "File '/content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data/random_forest.joblib' already exists...\n",
            "...Using the name '/content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data/random_forest(1).joblib' instead.\n",
            "Saving model to: /content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data/random_forest(1).joblib\n",
            "SUCCESS: Model saved to /content/drive/MyDrive/Colab Notebooks/COMP6002_Group10_Data/random_forest(1).joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network\n",
        "Recommend running this section in a runtime with GPU enabled."
      ],
      "metadata": {
        "id": "O4bSSnwGQfiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NN Setup\n",
        "Import and install required libraries, sets some initial values."
      ],
      "metadata": {
        "id": "236PhyQolLI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import PyTorch and confirm version\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "print(f\"Using PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# check the availability of and set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device.\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)"
      ],
      "metadata": {
        "id": "cHhG7NZ9lahV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install Skorch, providing a wrapper for using PyTorch with Sklearn\n",
        "!pip install skorch\n",
        "\n",
        "# import Skorch and confirm version\n",
        "from skorch import __version__ as skorch_version\n",
        "\n",
        "print(f\"Using Skorch version: {skorch_version}\")"
      ],
      "metadata": {
        "id": "GUSn5zCTQfGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Neural Network\n",
        "Currently using a Multilayer Perceptron (MLP), consider swapping to a hybrid model of a MLP and Convolutional Neural Network (CNN) later."
      ],
      "metadata": {
        "id": "AsNxj_cEDydF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_MLP(nn.Module):\n",
        "  \"\"\"Class that defines a multilayer perceptron model.\"\"\"\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    \"\"\"\n",
        "    Construct a new NN_MLP object.\n",
        "\n",
        "    Parameters:\n",
        "      input_size (int): number of neurons in the input layer, should be equal to the number of features in the data.\n",
        "      hidden_size (int): number of neurons in each hidden layer.\n",
        "      output_size (int): number of neurons in the output layer, should be equal to the number of possible classes.\n",
        "    Returns:\n",
        "      : no value returned.\n",
        "    \"\"\"\n",
        "    super(NN_MLP, self).__init__()\n",
        "    # layers\n",
        "    self.h1 = nn.Linear(input_size, hidden_size)\n",
        "    self.h2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.output = nn.Linear(hidden_size, output_size)\n",
        "    # activation functions\n",
        "    self.relu = nn.ReLu()\n",
        "    self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      X (Any): features to make prediction on.\n",
        "    Returns:\n",
        "      Any: predicted value.\n",
        "    \"\"\"\n",
        "    out = self.h1(X)\n",
        "    out = self.relu(out)\n",
        "    out = self.h2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.output(out)\n",
        "    out = self.softmax(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "OFTGLP7jknbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Parameters and Instantiate"
      ],
      "metadata": {
        "id": "RdrNQssiucoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skorch import NeuralNetClassifer\n",
        "from skorch.callbacks import EarlyStopping\n",
        "\n",
        "#optimizer = \"\"\n",
        "input_neurons = X_train_res.shape()[1]\n",
        "output_neurons = y_train_res.unique().size()\n",
        "\n",
        "# formulas for determining hidden layer sizes\n",
        "hidden_1 = ((input_neurons * (2/3)) + output_neurons)\n",
        "hidden_2 = X_train_res.shape()[0] // (2 * (input_neurons + output_neurons))\n",
        "\n",
        "# early stop callback\n",
        "early_stop = EarlyStopping(\n",
        "    monitor = 'f1_weighted',\n",
        "    lower_is_better = False,\n",
        "    patience = 5,\n",
        "    load_best = True\n",
        ")\n",
        "\n",
        "# instantiate neural network\n",
        "nn_estimator = NeuralNetClassifier(\n",
        "    module = NN_MLP,\n",
        "    module__input_size = input_neurons,\n",
        "    module__output_size = output_neurons,\n",
        "    lr = 0.01,\n",
        "    max_epochs = 20,\n",
        "    train_split = None,\n",
        "    callbacks = [early_stop],\n",
        "    verbose = 0,\n",
        "    device = device\n",
        ")\n",
        "\n",
        "# parameters for grid search\n",
        "nn_params = {\n",
        "    'lr' : [0.01, 3e-3],\n",
        "    'module__hidden_size' : [hidden_1, hidden_2]\n",
        "}\n",
        "\n",
        "nn_model = GridSearchCV(\n",
        "    estimator = nn_estimator,\n",
        "    param_grid = nn_params,\n",
        "    scoring = [\"accuracy\", \"f1_weighted\"],\n",
        "    refit = \"f1_weighted\",\n",
        "    cv = 5,\n",
        "    verbose = 3,\n",
        "    return_train_score = True,\n",
        "    n_jobs = 1\n",
        ")"
      ],
      "metadata": {
        "id": "986tIgh_VvL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Neural Network"
      ],
      "metadata": {
        "id": "xrVTPEvYuFKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracker.start_task(\"train nn\")\n",
        "\n",
        "try:\n",
        "  # train the neural network\n",
        "  nn_model.fit(X_train_res, y_train_res)\n",
        "finally:\n",
        "  nn_emissions = tracker.stop_task(\"train nn\")"
      ],
      "metadata": {
        "id": "bJzW9VXvhakV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Emissions: {nn_emissions} kg CO₂')"
      ],
      "metadata": {
        "id": "OP703wiWi_eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export Neural Network"
      ],
      "metadata": {
        "id": "HMdp3T_XuSk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SaveNN(nn_save = nn_model, nn_fpath = (save_dir + '/neural_network.pkl'), skip_prompt = True)"
      ],
      "metadata": {
        "id": "RrAeeGGGttLJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}